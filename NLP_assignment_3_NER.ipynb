{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP- assignment 3 - NER",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-WJBimYDLJS"
      },
      "source": [
        "# Assignment 3\n",
        "Training a simple neural named entity recognizer (NER)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3enPCGBF8FlX"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "from random import sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5QSIEoyDdWh"
      },
      "source": [
        "In this assignment you are required to build a full training and testing pipeline for a neural sequentail tagger for named entities, using LSTM.\n",
        "\n",
        "The dataset that you will be working on is called ReCoNLL 2003, which is a corrected version of the CoNLL 2003 dataset: https://www.clips.uantwerpen.be/conll2003/ner/\n",
        "\n",
        "\n",
        "The three files (train, test and eval) are available from the course git repository (https://github.com/kfirbar/nlp-course)\n",
        "\n",
        "As you can see, the annotated texts are labeled according to the IOB annotation scheme, for 3 entity types: Person, Organization, Location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ul2Y3vuPoV8"
      },
      "source": [
        "**Task 1:** Write a funtion *read_data* for reading the data from a single file (either train, test or eval). This function recieves a filepath and returns a list of sentence. Every sentence is encoded as a pair of lists, one list contains the words and one list contains the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prgzgtt8Jw4Y"
      },
      "source": [
        "def read_data(filepath):\n",
        "    f = open(filepath,'r')  \n",
        "    lines = f.readlines()\n",
        "    \n",
        "    data = []\n",
        "    words = []\n",
        "    tags  = []\n",
        "\n",
        "    for line in lines:        \n",
        "      if line == '\\n': # new sentence\n",
        "        data.append((words,tags)) \n",
        "        words = []\n",
        "        tags = []\n",
        "      else:\n",
        "        word, tag = line.split(' ', 1)\n",
        "        tag = tag.rstrip()\n",
        "        words.append(word)\n",
        "        tags.append(tag)\n",
        "    data.append((words,tags)) # last sentence\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHQA6Cxjhovq",
        "outputId": "36486f06-b004-4473-c530-ef33994969a4"
      },
      "source": [
        "!git clone https://github.com/kfirbar/nlp-course"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nlp-course'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 58 (delta 24), reused 31 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (58/58), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6E6SfEL0o3-"
      },
      "source": [
        "train = read_data('/content/nlp-course/connl03_train.txt')\n",
        "test = read_data('/content/nlp-course/connl03_test.txt')\n",
        "dev = read_data('/content/nlp-course/connl03_dev.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuGwk6OwRWGS"
      },
      "source": [
        "The following Vocab class can be served as a dictionary that maps words and tags into Ids. The UNK_TOKEN should be used for words that are not part of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rKIB5o_vQO8"
      },
      "source": [
        "UNK_TOKEN = 0\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.word2id = {\"__unk__\": UNK_TOKEN}\n",
        "        self.id2word = {UNK_TOKEN: \"__unk__\"}\n",
        "        self.n_words = 1\n",
        "        \n",
        "        self.tag2id = {\"O\":0, \"B-PER\":1, \"I-PER\": 2, \"B-LOC\": 3, \"I-LOC\": 4, \"B-ORG\": 5, \"I-ORG\": 6}\n",
        "        self.id2tag = {0:\"O\", 1:\"B-PER\", 2:\"I-PER\", 3:\"B-LOC\", 4:\"I-LOC\", 5:\"B-ORG\", 6:\"I-ORG\"}\n",
        "        \n",
        "    def index_words(self, words):\n",
        "      word_indexes = [self.index_word(w) for w in words]\n",
        "      return word_indexes\n",
        "\n",
        "    def index_tags(self, tags):\n",
        "      tag_indexes = [self.tag2id[t] for t in tags]\n",
        "      return tag_indexes\n",
        "    \n",
        "    def index_word(self, w):\n",
        "        if w not in self.word2id:\n",
        "            self.word2id[w] = self.n_words\n",
        "            self.id2word[self.n_words] = w\n",
        "            self.n_words += 1\n",
        "        return self.word2id[w]\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDKYryfKfNdh"
      },
      "source": [
        "**Task 2:** Write a function *prepare_data* that takes one of the [train, dev, test] and the Vocab instance, for converting each pair of (words,labels) to a pair of indexes (from Vocab). Each pair should be added to *data_sequences*, which is returned back from the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW5os9CRpdY0"
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noIY3zWKvhBd"
      },
      "source": [
        "def prepare_data(data, vocab):\n",
        "    data_sequences = []\n",
        "\n",
        "    for words, tags in data:\n",
        "      words_ids = vocab.index_words(words)\n",
        "      tags_ids = vocab.index_tags(tags)\n",
        "      data_sequences.append((words_ids, tags_ids))\n",
        "\n",
        "    return data_sequences, vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFV57SXtpakF"
      },
      "source": [
        "train_sequences, vocab = prepare_data(train, vocab)\n",
        "dev_sequences, vocab = prepare_data(dev, vocab)\n",
        "test_sequences, vocab = prepare_data(test, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UccfiRRtiEet"
      },
      "source": [
        "**Task 3:** Write NERNet, a PyTorch Module for labeling words with NER tags. \n",
        "\n",
        "*input_size:* the size of the vocabulary\n",
        "\n",
        "*embedding_size:* the size of the embeddings\n",
        "\n",
        "*hidden_size:* the LSTM hidden size\n",
        "\n",
        "*output_size:* the number tags we are predicting for\n",
        "\n",
        "*n_layers:* the number of layers we want to use in LSTM\n",
        "\n",
        "*directions:* could 1 or 2, indicating unidirectional or bidirectional LSTM, respectively\n",
        "\n",
        "The input for your forward function should be a single sentence tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke1LyUQNyQaM"
      },
      "source": [
        "class NERNet(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, n_layers, directions):\n",
        "        super(NERNet, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.directions = directions\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, bidirectional=(True if directions==2 else False))\n",
        "        self.out = nn.Linear(hidden_size*directions, output_size)\n",
        "    \n",
        "    def forward(self, input_sentence):\n",
        "        embedded = self.embedding(input_sentence)\n",
        "        lstm, _ = self.lstm(embedded.view(len(input_sentence), 1, -1))\n",
        "        output = self.out(lstm.view(len(input_sentence), -1))\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEGSQdeUkTP8"
      },
      "source": [
        "**Task 4:** write a training loop, which takes a model (instance of NERNet) and number of epochs to train on. The loss is always CrossEntropyLoss and the optimizer is always Adam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avkHfjT3k0HM"
      },
      "source": [
        "def train_loop(model, n_epochs):\n",
        "  # Loss function\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Optimizer (ADAM is a fancy version of SGD)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "  \n",
        "  train_loss = []\n",
        "\n",
        "  for e in range(1, n_epochs + 1):\n",
        "    train_shuff = sample(train_sequences, len(train_sequences))\n",
        "    running_loss = 0.0\n",
        "    for i, sequence in enumerate(train_shuff):\n",
        "      if len(sequence[0]) == 0:\n",
        "        continue\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad() \n",
        "      \n",
        "      # forward + backward + optimize\n",
        "      outputs = model(torch.LongTensor(sequence[0]).cuda())\n",
        "      loss = criterion(outputs, torch.LongTensor(sequence[1]).cuda())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      if (i+1) % 500 == 0:   \n",
        "          print('[%d, %5d] loss: %.3f' %\n",
        "                (e, i + 1, running_loss / 500))\n",
        "          train_loss.append(running_loss / 500)\n",
        "          running_loss = 0.0\n",
        "\n",
        "  print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baN1c_B7lTjb"
      },
      "source": [
        "**Task 5:** write an evaluation loop on a trained model, using the dev and test datasets. This function print the true positive rate (TPR), also known as Recall and the opposite to false positive rate (FPR), also known as precision, of each label seperately (7 labels in total), and for all the 6 labels (except O) together. The caption argument for the function should be served for printing, so that when you print include it as a prefix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyQAjGaqmd8U"
      },
      "source": [
        "def evaluate(model, caption):\n",
        "  print(caption)\n",
        "  \n",
        "  # dev\n",
        "  print(\"dev\")\n",
        "  all_predicted = []\n",
        "  all_tags = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for sequence in dev_sequences:\n",
        "      if len(sequence[0]) == 0:\n",
        "        continue\n",
        "      predicted = model(torch.LongTensor(sequence[0]).cuda()).max(1).indices\n",
        "      all_predicted.append(predicted.tolist())\n",
        "      all_tags.append(sequence[1])\n",
        "    \n",
        "    all_predicted = [prediction for sublist in all_predicted for prediction in sublist]\n",
        "    all_tags = [tag for sublist in all_tags for tag in sublist]\n",
        "\n",
        "    # calculate recall = TP/(TP+FN) = TP/P and precision = TP/(TP+FP)\n",
        "    TP = np.zeros(7)\n",
        "    P = np.zeros(7)\n",
        "    FP = np.zeros(7)\n",
        "\n",
        "    for i in range(0,len(all_predicted)):\n",
        "      P[all_tags[i]] += 1\n",
        "      if all_tags[i] == all_predicted[i]:\n",
        "        TP[all_tags[i]] += 1\n",
        "      if all_tags[i] != all_predicted[i]:\n",
        "        FP[all_predicted[i]] += 1\n",
        "    \n",
        "    recall = np.divide(TP,P)\n",
        "    precision = np.divide(TP, np.add(TP,FP))\n",
        "\n",
        "    # calculate accuracy\n",
        "    correct = 0.0\n",
        "    counter = 0.0\n",
        "\n",
        "    for i in range(0,len(all_predicted)):\n",
        "      if all_tags[i] == 0:\n",
        "        continue\n",
        "      counter += 1\n",
        "      if all_tags[i] == all_predicted[i]:\n",
        "        correct += 1\n",
        "    \n",
        "    accuracy = correct / counter\n",
        "    \n",
        "    # print evaluation\n",
        "    temp = np.concatenate((np.reshape(recall, (7,1)), np.reshape(precision, (7,1))), axis=1)\n",
        "    df = pd.DataFrame(data=temp, index=['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG'], columns=['recall', 'precision'])\n",
        "    print(df)\n",
        "    print(\"accuracy: \", accuracy)\n",
        "\n",
        "  # test\n",
        "  print(\"test\")\n",
        "  all_predicted = []\n",
        "  all_tags = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for sequence in test_sequences:\n",
        "      if len(sequence[0]) == 0:\n",
        "        continue\n",
        "      predicted = model(torch.LongTensor(sequence[0]).cuda()).max(1).indices\n",
        "      all_predicted.append(predicted.tolist())\n",
        "      all_tags.append(sequence[1])\n",
        "    \n",
        "    all_predicted = [prediction for sublist in all_predicted for prediction in sublist]\n",
        "    all_tags = [tag for sublist in all_tags for tag in sublist]\n",
        "\n",
        "    # calculate recall = TP/(TP+FN) = TP/P and precision = TP/(TP+FP)\n",
        "    TP = np.zeros(7)\n",
        "    P = np.zeros(7)\n",
        "    FP = np.zeros(7)\n",
        "\n",
        "    for i in range(0,len(all_predicted)):\n",
        "      P[all_tags[i]] += 1\n",
        "      if all_tags[i] == all_predicted[i]:\n",
        "        TP[all_tags[i]] += 1\n",
        "      if all_tags[i] != all_predicted[i]:\n",
        "        FP[all_predicted[i]] += 1\n",
        "    \n",
        "    recall = np.divide(TP,P)\n",
        "    precision = np.divide(TP, np.add(TP,FP))\n",
        "\n",
        "    # calculate accuracy\n",
        "    correct = 0.0\n",
        "    counter = 0.0\n",
        "\n",
        "    for i in range(0,len(all_predicted)):\n",
        "      if all_tags[i] == 0:\n",
        "        continue\n",
        "      counter += 1\n",
        "      if all_tags[i] == all_predicted[i]:\n",
        "        correct += 1\n",
        "    \n",
        "    accuracy = correct / counter\n",
        "    \n",
        "    # print evaluation\n",
        "    temp = np.concatenate((np.reshape(recall, (7,1)), np.reshape(precision, (7,1))), axis=1)\n",
        "    df = pd.DataFrame(data=temp, index=['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG'], columns=['recall', 'precision'])\n",
        "    print(df)\n",
        "    print(\"accuracy: \", accuracy, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQSXqWNOmqG4"
      },
      "source": [
        "**Task 6:** Train and evaluate a few models, all with embedding_size=300, and with the following hyper parameters (you may use that as captions for the models as well):\n",
        "\n",
        "Model 1: (hidden_size: 500, n_layers: 1, directions: 1)\n",
        "\n",
        "Model 2: (hidden_size: 500, n_layers: 2, directions: 1)\n",
        "\n",
        "Model 3: (hidden_size: 500, n_layers: 3, directions: 1)\n",
        "\n",
        "Model 4: (hidden_size: 500, n_layers: 1, directions: 2)\n",
        "\n",
        "Model 5: (hidden_size: 500, n_layers: 2, directions: 2)\n",
        "\n",
        "Model 6: (hidden_size: 500, n_layers: 3, directions: 2)\n",
        "\n",
        "Model 4: (hidden_size: 800, n_layers: 1, directions: 2)\n",
        "\n",
        "Model 5: (hidden_size: 800, n_layers: 2, directions: 2)\n",
        "\n",
        "Model 6: (hidden_size: 800, n_layers: 3, directions: 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTNmBU6hycZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9cb7bf-b5f5-495c-a2c4-841491e6d3b7"
      },
      "source": [
        "input_size = len(vocab.word2id)\n",
        "output_size = 7\n",
        "embedding_size = 300\n",
        "\n",
        "model1 = NERNet(input_size, embedding_size, 500, output_size, 1, 1).cuda()\n",
        "print(\"model 1 - start training\")\n",
        "train_loop(model1, 10)\n",
        "\n",
        "model2 = NERNet(input_size, embedding_size, 500, output_size, 2, 1).cuda()\n",
        "print(\"model 2 - start training\")\n",
        "train_loop(model2, 10)\n",
        "\n",
        "model3 = NERNet(input_size, embedding_size, 500, output_size, 3, 1).cuda()\n",
        "print(\"model 3 - start training\")\n",
        "train_loop(model3, 10)\n",
        "\n",
        "model4 = NERNet(input_size, embedding_size, 500, output_size, 1, 2).cuda()\n",
        "print(\"model 4 - start training\")\n",
        "train_loop(model4, 10)\n",
        "\n",
        "model5 = NERNet(input_size, embedding_size, 500, output_size, 2, 2).cuda()\n",
        "print(\"model 5 - start training\")\n",
        "train_loop(model5, 10)\n",
        "\n",
        "model6 = NERNet(input_size, embedding_size, 500, output_size, 3, 2).cuda()\n",
        "print(\"model 6 - start training\")\n",
        "train_loop(model6, 10)\n",
        "\n",
        "model7 = NERNet(input_size, embedding_size, 800, output_size, 1, 2).cuda()\n",
        "print(\"model 7 - start training\")\n",
        "train_loop(model7, 10)\n",
        "\n",
        "model8 = NERNet(input_size, embedding_size, 800, output_size, 2, 2).cuda()\n",
        "print(\"model 8 - start training\")\n",
        "train_loop(model8, 10)\n",
        "\n",
        "model9 = NERNet(input_size, embedding_size, 800, output_size, 3, 2).cuda()\n",
        "print(\"model 9 - start training\")\n",
        "train_loop(model9, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model 1 - start training\n",
            "[1,   500] loss: 2.963\n",
            "[1,  1000] loss: 2.245\n",
            "[1,  1500] loss: 1.973\n",
            "[2,   500] loss: 1.574\n",
            "[2,  1000] loss: 1.336\n",
            "[2,  1500] loss: 1.243\n",
            "[3,   500] loss: 0.966\n",
            "[3,  1000] loss: 0.877\n",
            "[3,  1500] loss: 0.851\n",
            "[4,   500] loss: 0.602\n",
            "[4,  1000] loss: 0.588\n",
            "[4,  1500] loss: 0.596\n",
            "[5,   500] loss: 0.422\n",
            "[5,  1000] loss: 0.397\n",
            "[5,  1500] loss: 0.405\n",
            "[6,   500] loss: 0.252\n",
            "[6,  1000] loss: 0.257\n",
            "[6,  1500] loss: 0.264\n",
            "[7,   500] loss: 0.153\n",
            "[7,  1000] loss: 0.165\n",
            "[7,  1500] loss: 0.177\n",
            "[8,   500] loss: 0.099\n",
            "[8,  1000] loss: 0.110\n",
            "[8,  1500] loss: 0.096\n",
            "[9,   500] loss: 0.059\n",
            "[9,  1000] loss: 0.065\n",
            "[9,  1500] loss: 0.068\n",
            "[10,   500] loss: 0.042\n",
            "[10,  1000] loss: 0.035\n",
            "[10,  1500] loss: 0.043\n",
            "Finished Training\n",
            "model 2 - start training\n",
            "[1,   500] loss: 2.866\n",
            "[1,  1000] loss: 2.192\n",
            "[1,  1500] loss: 1.845\n",
            "[2,   500] loss: 1.321\n",
            "[2,  1000] loss: 1.230\n",
            "[2,  1500] loss: 1.157\n",
            "[3,   500] loss: 0.792\n",
            "[3,  1000] loss: 0.729\n",
            "[3,  1500] loss: 0.690\n",
            "[4,   500] loss: 0.442\n",
            "[4,  1000] loss: 0.413\n",
            "[4,  1500] loss: 0.394\n",
            "[5,   500] loss: 0.222\n",
            "[5,  1000] loss: 0.210\n",
            "[5,  1500] loss: 0.211\n",
            "[6,   500] loss: 0.121\n",
            "[6,  1000] loss: 0.118\n",
            "[6,  1500] loss: 0.090\n",
            "[7,   500] loss: 0.064\n",
            "[7,  1000] loss: 0.063\n",
            "[7,  1500] loss: 0.051\n",
            "[8,   500] loss: 0.075\n",
            "[8,  1000] loss: 0.030\n",
            "[8,  1500] loss: 0.037\n",
            "[9,   500] loss: 0.025\n",
            "[9,  1000] loss: 0.019\n",
            "[9,  1500] loss: 0.025\n",
            "[10,   500] loss: 0.018\n",
            "[10,  1000] loss: 0.015\n",
            "[10,  1500] loss: 0.016\n",
            "Finished Training\n",
            "model 3 - start training\n",
            "[1,   500] loss: 2.845\n",
            "[1,  1000] loss: 2.341\n",
            "[1,  1500] loss: 1.927\n",
            "[2,   500] loss: 1.373\n",
            "[2,  1000] loss: 1.228\n",
            "[2,  1500] loss: 1.121\n",
            "[3,   500] loss: 0.768\n",
            "[3,  1000] loss: 0.754\n",
            "[3,  1500] loss: 0.705\n",
            "[4,   500] loss: 0.437\n",
            "[4,  1000] loss: 0.425\n",
            "[4,  1500] loss: 0.427\n",
            "[5,   500] loss: 0.249\n",
            "[5,  1000] loss: 0.232\n",
            "[5,  1500] loss: 0.237\n",
            "[6,   500] loss: 0.145\n",
            "[6,  1000] loss: 0.153\n",
            "[6,  1500] loss: 0.140\n",
            "[7,   500] loss: 0.068\n",
            "[7,  1000] loss: 0.088\n",
            "[7,  1500] loss: 0.088\n",
            "[8,   500] loss: 0.071\n",
            "[8,  1000] loss: 0.064\n",
            "[8,  1500] loss: 0.052\n",
            "[9,   500] loss: 0.042\n",
            "[9,  1000] loss: 0.048\n",
            "[9,  1500] loss: 0.068\n",
            "[10,   500] loss: 0.027\n",
            "[10,  1000] loss: 0.035\n",
            "[10,  1500] loss: 0.029\n",
            "Finished Training\n",
            "model 4 - start training\n",
            "[1,   500] loss: 2.743\n",
            "[1,  1000] loss: 1.882\n",
            "[1,  1500] loss: 1.475\n",
            "[2,   500] loss: 1.104\n",
            "[2,  1000] loss: 0.878\n",
            "[2,  1500] loss: 0.800\n",
            "[3,   500] loss: 0.563\n",
            "[3,  1000] loss: 0.541\n",
            "[3,  1500] loss: 0.499\n",
            "[4,   500] loss: 0.324\n",
            "[4,  1000] loss: 0.295\n",
            "[4,  1500] loss: 0.318\n",
            "[5,   500] loss: 0.156\n",
            "[5,  1000] loss: 0.170\n",
            "[5,  1500] loss: 0.149\n",
            "[6,   500] loss: 0.082\n",
            "[6,  1000] loss: 0.076\n",
            "[6,  1500] loss: 0.086\n",
            "[7,   500] loss: 0.042\n",
            "[7,  1000] loss: 0.037\n",
            "[7,  1500] loss: 0.042\n",
            "[8,   500] loss: 0.018\n",
            "[8,  1000] loss: 0.025\n",
            "[8,  1500] loss: 0.024\n",
            "[9,   500] loss: 0.010\n",
            "[9,  1000] loss: 0.010\n",
            "[9,  1500] loss: 0.022\n",
            "[10,   500] loss: 0.009\n",
            "[10,  1000] loss: 0.015\n",
            "[10,  1500] loss: 0.010\n",
            "Finished Training\n",
            "model 5 - start training\n",
            "[1,   500] loss: 2.618\n",
            "[1,  1000] loss: 1.596\n",
            "[1,  1500] loss: 1.257\n",
            "[2,   500] loss: 0.821\n",
            "[2,  1000] loss: 0.681\n",
            "[2,  1500] loss: 0.609\n",
            "[3,   500] loss: 0.317\n",
            "[3,  1000] loss: 0.318\n",
            "[3,  1500] loss: 0.322\n",
            "[4,   500] loss: 0.147\n",
            "[4,  1000] loss: 0.113\n",
            "[4,  1500] loss: 0.131\n",
            "[5,   500] loss: 0.059\n",
            "[5,  1000] loss: 0.053\n",
            "[5,  1500] loss: 0.049\n",
            "[6,   500] loss: 0.018\n",
            "[6,  1000] loss: 0.028\n",
            "[6,  1500] loss: 0.021\n",
            "[7,   500] loss: 0.028\n",
            "[7,  1000] loss: 0.030\n",
            "[7,  1500] loss: 0.021\n",
            "[8,   500] loss: 0.012\n",
            "[8,  1000] loss: 0.035\n",
            "[8,  1500] loss: 0.036\n",
            "[9,   500] loss: 0.015\n",
            "[9,  1000] loss: 0.012\n",
            "[9,  1500] loss: 0.011\n",
            "[10,   500] loss: 0.011\n",
            "[10,  1000] loss: 0.006\n",
            "[10,  1500] loss: 0.016\n",
            "Finished Training\n",
            "model 6 - start training\n",
            "[1,   500] loss: 2.555\n",
            "[1,  1000] loss: 1.771\n",
            "[1,  1500] loss: 1.382\n",
            "[2,   500] loss: 0.782\n",
            "[2,  1000] loss: 0.664\n",
            "[2,  1500] loss: 0.663\n",
            "[3,   500] loss: 0.332\n",
            "[3,  1000] loss: 0.316\n",
            "[3,  1500] loss: 0.338\n",
            "[4,   500] loss: 0.150\n",
            "[4,  1000] loss: 0.141\n",
            "[4,  1500] loss: 0.140\n",
            "[5,   500] loss: 0.106\n",
            "[5,  1000] loss: 0.089\n",
            "[5,  1500] loss: 0.074\n",
            "[6,   500] loss: 0.046\n",
            "[6,  1000] loss: 0.056\n",
            "[6,  1500] loss: 0.058\n",
            "[7,   500] loss: 0.034\n",
            "[7,  1000] loss: 0.056\n",
            "[7,  1500] loss: 0.040\n",
            "[8,   500] loss: 0.019\n",
            "[8,  1000] loss: 0.028\n",
            "[8,  1500] loss: 0.041\n",
            "[9,   500] loss: 0.034\n",
            "[9,  1000] loss: 0.018\n",
            "[9,  1500] loss: 0.011\n",
            "[10,   500] loss: 0.013\n",
            "[10,  1000] loss: 0.024\n",
            "[10,  1500] loss: 0.042\n",
            "Finished Training\n",
            "model 7 - start training\n",
            "[1,   500] loss: 2.458\n",
            "[1,  1000] loss: 1.626\n",
            "[1,  1500] loss: 1.422\n",
            "[2,   500] loss: 0.916\n",
            "[2,  1000] loss: 0.836\n",
            "[2,  1500] loss: 0.800\n",
            "[3,   500] loss: 0.512\n",
            "[3,  1000] loss: 0.478\n",
            "[3,  1500] loss: 0.427\n",
            "[4,   500] loss: 0.268\n",
            "[4,  1000] loss: 0.278\n",
            "[4,  1500] loss: 0.271\n",
            "[5,   500] loss: 0.134\n",
            "[5,  1000] loss: 0.145\n",
            "[5,  1500] loss: 0.154\n",
            "[6,   500] loss: 0.060\n",
            "[6,  1000] loss: 0.065\n",
            "[6,  1500] loss: 0.078\n",
            "[7,   500] loss: 0.039\n",
            "[7,  1000] loss: 0.033\n",
            "[7,  1500] loss: 0.035\n",
            "[8,   500] loss: 0.011\n",
            "[8,  1000] loss: 0.012\n",
            "[8,  1500] loss: 0.018\n",
            "[9,   500] loss: 0.009\n",
            "[9,  1000] loss: 0.022\n",
            "[9,  1500] loss: 0.010\n",
            "[10,   500] loss: 0.018\n",
            "[10,  1000] loss: 0.009\n",
            "[10,  1500] loss: 0.012\n",
            "Finished Training\n",
            "model 8 - start training\n",
            "[1,   500] loss: 2.286\n",
            "[1,  1000] loss: 1.505\n",
            "[1,  1500] loss: 1.152\n",
            "[2,   500] loss: 0.700\n",
            "[2,  1000] loss: 0.626\n",
            "[2,  1500] loss: 0.618\n",
            "[3,   500] loss: 0.303\n",
            "[3,  1000] loss: 0.297\n",
            "[3,  1500] loss: 0.302\n",
            "[4,   500] loss: 0.149\n",
            "[4,  1000] loss: 0.127\n",
            "[4,  1500] loss: 0.130\n",
            "[5,   500] loss: 0.064\n",
            "[5,  1000] loss: 0.062\n",
            "[5,  1500] loss: 0.074\n",
            "[6,   500] loss: 0.024\n",
            "[6,  1000] loss: 0.020\n",
            "[6,  1500] loss: 0.033\n",
            "[7,   500] loss: 0.021\n",
            "[7,  1000] loss: 0.048\n",
            "[7,  1500] loss: 0.047\n",
            "[8,   500] loss: 0.034\n",
            "[8,  1000] loss: 0.025\n",
            "[8,  1500] loss: 0.015\n",
            "[9,   500] loss: 0.011\n",
            "[9,  1000] loss: 0.014\n",
            "[9,  1500] loss: 0.013\n",
            "[10,   500] loss: 0.057\n",
            "[10,  1000] loss: 0.037\n",
            "[10,  1500] loss: 0.023\n",
            "Finished Training\n",
            "model 9 - start training\n",
            "[1,   500] loss: 2.419\n",
            "[1,  1000] loss: 1.554\n",
            "[1,  1500] loss: 1.188\n",
            "[2,   500] loss: 0.727\n",
            "[2,  1000] loss: 0.684\n",
            "[2,  1500] loss: 0.589\n",
            "[3,   500] loss: 0.321\n",
            "[3,  1000] loss: 0.382\n",
            "[3,  1500] loss: 0.393\n",
            "[4,   500] loss: 0.196\n",
            "[4,  1000] loss: 0.187\n",
            "[4,  1500] loss: 0.212\n",
            "[5,   500] loss: 0.098\n",
            "[5,  1000] loss: 0.110\n",
            "[5,  1500] loss: 0.108\n",
            "[6,   500] loss: 0.045\n",
            "[6,  1000] loss: 0.040\n",
            "[6,  1500] loss: 0.070\n",
            "[7,   500] loss: 0.034\n",
            "[7,  1000] loss: 0.103\n",
            "[7,  1500] loss: 0.083\n",
            "[8,   500] loss: 0.020\n",
            "[8,  1000] loss: 0.053\n",
            "[8,  1500] loss: 0.055\n",
            "[9,   500] loss: 0.020\n",
            "[9,  1000] loss: 0.022\n",
            "[9,  1500] loss: 0.020\n",
            "[10,   500] loss: 0.020\n",
            "[10,  1000] loss: 0.063\n",
            "[10,  1500] loss: 0.028\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP9n3IcelqRa",
        "outputId": "60ad05cd-0955-4262-f634-a42a2ff09d45"
      },
      "source": [
        "evaluate(model1, \"hidden_size: 500, n_layers: 1, directions: 1\")\n",
        "evaluate(model2, \"hidden_size: 500, n_layers: 2, directions: 1\")\n",
        "evaluate(model3, \"hidden_size: 500, n_layers: 3, directions: 1\")\n",
        "evaluate(model4, \"hidden_size: 500, n_layers: 1, directions: 2\")\n",
        "evaluate(model5, \"hidden_size: 500, n_layers: 2, directions: 2\")\n",
        "evaluate(model6, \"hidden_size: 500, n_layers: 3, directions: 2\")\n",
        "evaluate(model7, \"hidden_size: 800, n_layers: 1, directions: 2\")\n",
        "evaluate(model8, \"hidden_size: 800, n_layers: 2, directions: 2\")\n",
        "evaluate(model9, \"hidden_size: 800, n_layers: 3, directions: 2\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden_size: 500, n_layers: 1, directions: 1\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.946059   0.925434\n",
            "B-PER  0.675000   0.572034\n",
            "I-PER  0.675159   0.736111\n",
            "B-LOC  0.688525   0.759036\n",
            "I-LOC  0.434783   0.833333\n",
            "B-ORG  0.589286   0.626582\n",
            "I-ORG  0.387931   0.725806\n",
            "accuracy:  0.615112160566706\n",
            "test\n",
            "         recall  precision\n",
            "O      0.940155   0.937016\n",
            "B-PER  0.702765   0.571161\n",
            "I-PER  0.706081   0.694352\n",
            "B-LOC  0.702624   0.800664\n",
            "I-LOC  0.471698   0.833333\n",
            "B-ORG  0.554286   0.552707\n",
            "I-ORG  0.400000   0.583942\n",
            "accuracy:  0.6288782816229117 \n",
            "\n",
            "hidden_size: 500, n_layers: 2, directions: 1\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.964793   0.929661\n",
            "B-PER  0.685000   0.774011\n",
            "I-PER  0.713376   0.888889\n",
            "B-LOC  0.754098   0.793103\n",
            "I-LOC  0.391304   0.642857\n",
            "B-ORG  0.571429   0.585366\n",
            "I-ORG  0.413793   0.640000\n",
            "accuracy:  0.6375442739079102\n",
            "test\n",
            "         recall  precision\n",
            "O      0.962692   0.933550\n",
            "B-PER  0.672811   0.724566\n",
            "I-PER  0.699324   0.778195\n",
            "B-LOC  0.758017   0.738636\n",
            "I-LOC  0.528302   0.823529\n",
            "B-ORG  0.565714   0.647059\n",
            "I-ORG  0.310000   0.563636\n",
            "accuracy:  0.6247016706443914 \n",
            "\n",
            "hidden_size: 500, n_layers: 3, directions: 1\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.967054   0.939441\n",
            "B-PER  0.675000   0.745856\n",
            "I-PER  0.732484   0.891473\n",
            "B-LOC  0.726776   0.831250\n",
            "I-LOC  0.478261   0.846154\n",
            "B-ORG  0.613095   0.602339\n",
            "I-ORG  0.482759   0.549020\n",
            "accuracy:  0.6528925619834711\n",
            "test\n",
            "         recall  precision\n",
            "O      0.959190   0.941977\n",
            "B-PER  0.688940   0.764706\n",
            "I-PER  0.736486   0.795620\n",
            "B-LOC  0.749271   0.883162\n",
            "I-LOC  0.490566   0.928571\n",
            "B-ORG  0.588571   0.553763\n",
            "I-ORG  0.470000   0.470000\n",
            "accuracy:  0.6563245823389021 \n",
            "\n",
            "hidden_size: 500, n_layers: 1, directions: 2\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.976421   0.925881\n",
            "B-PER  0.685000   0.740541\n",
            "I-PER  0.707006   0.853846\n",
            "B-LOC  0.704918   0.781818\n",
            "I-LOC  0.304348   0.875000\n",
            "B-ORG  0.571429   0.721805\n",
            "I-ORG  0.387931   0.789474\n",
            "accuracy:  0.6198347107438017\n",
            "test\n",
            "         recall  precision\n",
            "O      0.976702   0.933760\n",
            "B-PER  0.691244   0.791557\n",
            "I-PER  0.702703   0.806202\n",
            "B-LOC  0.743440   0.812102\n",
            "I-LOC  0.490566   0.896552\n",
            "B-ORG  0.594286   0.750903\n",
            "I-ORG  0.375000   0.641026\n",
            "accuracy:  0.639618138424821 \n",
            "\n",
            "hidden_size: 500, n_layers: 2, directions: 2\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.976744   0.942350\n",
            "B-PER  0.705000   0.815029\n",
            "I-PER  0.738854   0.758170\n",
            "B-LOC  0.808743   0.813187\n",
            "I-LOC  0.391304   0.818182\n",
            "B-ORG  0.678571   0.786207\n",
            "I-ORG  0.456897   0.757143\n",
            "accuracy:  0.6859504132231405\n",
            "test\n",
            "         recall  precision\n",
            "O      0.977463   0.946336\n",
            "B-PER  0.730415   0.892958\n",
            "I-PER  0.841216   0.792994\n",
            "B-LOC  0.822157   0.785515\n",
            "I-LOC  0.622642   0.825000\n",
            "B-ORG  0.637143   0.802158\n",
            "I-ORG  0.415000   0.728070\n",
            "accuracy:  0.7082338902147971 \n",
            "\n",
            "hidden_size: 500, n_layers: 3, directions: 2\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.986434   0.935949\n",
            "B-PER  0.710000   0.871166\n",
            "I-PER  0.757962   0.922481\n",
            "B-LOC  0.732240   0.858974\n",
            "I-LOC  0.478261   0.733333\n",
            "B-ORG  0.625000   0.750000\n",
            "I-ORG  0.534483   0.805195\n",
            "accuracy:  0.6765053128689492\n",
            "test\n",
            "         recall  precision\n",
            "O      0.979290   0.938695\n",
            "B-PER  0.705069   0.833787\n",
            "I-PER  0.763514   0.879377\n",
            "B-LOC  0.728863   0.922509\n",
            "I-LOC  0.528302   0.875000\n",
            "B-ORG  0.642857   0.690184\n",
            "I-ORG  0.480000   0.690647\n",
            "accuracy:  0.6748210023866349 \n",
            "\n",
            "hidden_size: 800, n_layers: 1, directions: 2\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.975129   0.929209\n",
            "B-PER  0.685000   0.717277\n",
            "I-PER  0.713376   0.829630\n",
            "B-LOC  0.759563   0.794286\n",
            "I-LOC  0.391304   1.000000\n",
            "B-ORG  0.583333   0.759690\n",
            "I-ORG  0.387931   0.818182\n",
            "accuracy:  0.6375442739079102\n",
            "test\n",
            "         recall  precision\n",
            "O      0.974418   0.931984\n",
            "B-PER  0.705069   0.768844\n",
            "I-PER  0.719595   0.797753\n",
            "B-LOC  0.708455   0.773885\n",
            "I-LOC  0.547170   0.935484\n",
            "B-ORG  0.565714   0.738806\n",
            "I-ORG  0.315000   0.636364\n",
            "accuracy:  0.6276849642004774 \n",
            "\n",
            "hidden_size: 800, n_layers: 2, directions: 2\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.982881   0.936020\n",
            "B-PER  0.705000   0.805714\n",
            "I-PER  0.770701   0.864286\n",
            "B-LOC  0.737705   0.870968\n",
            "I-LOC  0.391304   0.818182\n",
            "B-ORG  0.648810   0.773050\n",
            "I-ORG  0.474138   0.785714\n",
            "accuracy:  0.6729634002361276\n",
            "test\n",
            "         recall  precision\n",
            "O      0.978834   0.943767\n",
            "B-PER  0.760369   0.808824\n",
            "I-PER  0.777027   0.842491\n",
            "B-LOC  0.749271   0.829032\n",
            "I-LOC  0.547170   0.935484\n",
            "B-ORG  0.631429   0.780919\n",
            "I-ORG  0.460000   0.724409\n",
            "accuracy:  0.6915274463007159 \n",
            "\n",
            "hidden_size: 800, n_layers: 3, directions: 2\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.965116   0.944673\n",
            "B-PER  0.750000   0.697674\n",
            "I-PER  0.745223   0.764706\n",
            "B-LOC  0.726776   0.841772\n",
            "I-LOC  0.434783   0.588235\n",
            "B-ORG  0.678571   0.726115\n",
            "I-ORG  0.508621   0.737500\n",
            "accuracy:  0.6883116883116883\n",
            "test\n",
            "         recall  precision\n",
            "O      0.964215   0.944652\n",
            "B-PER  0.725806   0.715909\n",
            "I-PER  0.756757   0.777778\n",
            "B-LOC  0.693878   0.812287\n",
            "I-LOC  0.584906   0.704545\n",
            "B-ORG  0.651429   0.705882\n",
            "I-ORG  0.495000   0.651316\n",
            "accuracy:  0.6772076372315036 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM74r0_8nk5s"
      },
      "source": [
        "**Task 6:** Download the GloVe embeddings from https://nlp.stanford.edu/projects/glove/ (use the 300-dim vectors from glove.6B.zip). Then intialize the nn.Embedding module in your NERNet with these embeddings, so that you can start your training with pre-trained vectors. Repeat Task 6 and print the results for each model.\n",
        "\n",
        "Note: make sure that vectors are aligned with the IDs in your Vocab, in other words, make sure that for example the word with ID 0 is the first vector in the GloVe matrix of vectors that you initialize nn.Embedding with. For a dicussion on how to do that, check it this link:\n",
        "https://discuss.pytorch.org/t/can-we-use-pre-trained-word-embeddings-for-weight-initialization-in-nn-embedding/1222"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRiMbvx9o5Rh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df396d79-66fa-410a-93dc-a354ac086c8f"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-03 08:26:05--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-06-03 08:26:05--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-06-03 08:26:05--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.16MB/s    in 2m 40s  \n",
            "\n",
            "2021-06-03 08:28:45 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzdW2tEzwqan",
        "outputId": "bf48b113-4d7e-4d81-b68d-6d8ea6f92fcd"
      },
      "source": [
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqivGxskwsSc"
      },
      "source": [
        "# use the 300-dim vectors from glove.6B.zip\n",
        "!rm glove.6B.50d.txt\n",
        "!rm glove.6B.100d.txt\n",
        "!rm glove.6B.200d.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0A8OMyia5pM"
      },
      "source": [
        "def load_glove_embeddings(path, word2id):\n",
        "  # creats embeddings tensor and makes sure that embedding vectors are aligned with the IDs in the vocabulary\n",
        "  with open(path, 'r') as f:\n",
        "      embeddings = np.zeros((len(word2id), 300))\n",
        "      for line in f.readlines():\n",
        "          temp = line.split()\n",
        "          word = temp[0]\n",
        "          id = word2id.get(word)\n",
        "          if id:\n",
        "              embed = np.array(temp[1:], dtype='float32')\n",
        "              embeddings[id] = embed\n",
        "      return torch.from_numpy(embeddings).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIqLWGpPcao4"
      },
      "source": [
        "glove_embeddings = load_glove_embeddings('glove.6B.300d.txt', vocab.word2id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVKTGCTcduQ9",
        "outputId": "d63e5e8c-f439-405b-c18b-594e23faa8d3"
      },
      "source": [
        "glove_embeddings.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8955, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oTZa081W_rBs",
        "outputId": "3224e534-cd62-4f97-8fb5-0004556fb03a"
      },
      "source": [
        "glove_embeddings.type()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'torch.FloatTensor'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGOztF5GeHPi"
      },
      "source": [
        "class NERNetGlove(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, n_layers, directions):\n",
        "        super(NERNetGlove, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.directions = directions\n",
        "\n",
        "        self.embedding = nn.Embedding(glove_embeddings.size(0), glove_embeddings.size(1))\n",
        "        self.embedding.weight = nn.Parameter(glove_embeddings)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, bidirectional=(True if directions==2 else False))\n",
        "        self.out = nn.Linear(hidden_size*directions, output_size)\n",
        "    \n",
        "    def forward(self, input_sentence):\n",
        "        embedded = self.embedding(input_sentence)\n",
        "        lstm, _ = self.lstm(embedded.view(len(input_sentence), 1, -1))\n",
        "        output = self.out(lstm.view(len(input_sentence), -1))\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMt4_Wczen01"
      },
      "source": [
        "def train_loop_glove(model, n_epochs):\n",
        "  # Loss function\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Optimizer (ADAM is a fancy version of SGD)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "  \n",
        "  train_loss = []\n",
        "\n",
        "  for e in range(1, n_epochs + 1):\n",
        "    train_shuff = sample(train_sequences, len(train_sequences))\n",
        "    running_loss = 0.0\n",
        "    for i, sequence in enumerate(train_shuff):\n",
        "      if len(sequence[0]) == 0:\n",
        "        continue\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad() \n",
        "      \n",
        "      # forward + backward + optimize\n",
        "      outputs = model(torch.LongTensor(sequence[0]).cuda())\n",
        "      loss = criterion(outputs, torch.LongTensor(sequence[1]).cuda())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      if (i+1) % 500 == 0:   \n",
        "          print('[%d, %5d] loss: %.3f' %\n",
        "                (e, i + 1, running_loss / 500))\n",
        "          train_loss.append(running_loss / 500)\n",
        "          running_loss = 0.0\n",
        "\n",
        "  print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdYTl3R1et4K"
      },
      "source": [
        "def evaluate_glove(model, caption):\n",
        "  print(caption)\n",
        "  \n",
        "  # dev\n",
        "  print(\"dev\")\n",
        "  all_predicted = []\n",
        "  all_tags = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for sequence in dev_sequences:\n",
        "      if len(sequence[0]) == 0:\n",
        "        continue\n",
        "      predicted = model(torch.LongTensor(sequence[0]).cuda()).max(1).indices\n",
        "      all_predicted.append(predicted.tolist())\n",
        "      all_tags.append(sequence[1])\n",
        "    \n",
        "    all_predicted = [prediction for sublist in all_predicted for prediction in sublist]\n",
        "    all_tags = [tag for sublist in all_tags for tag in sublist]\n",
        "\n",
        "    # calculate recall = TP/(TP+FN) = TP/P and precision = TP/(TP+FP)\n",
        "    TP = np.zeros(7)\n",
        "    P = np.zeros(7)\n",
        "    FP = np.zeros(7)\n",
        "\n",
        "    for i in range(0,len(all_predicted)):\n",
        "      P[all_tags[i]] += 1\n",
        "      if all_tags[i] == all_predicted[i]:\n",
        "        TP[all_tags[i]] += 1\n",
        "      if all_tags[i] != all_predicted[i]:\n",
        "        FP[all_predicted[i]] += 1\n",
        "    \n",
        "    recall = np.divide(TP,P)\n",
        "    precision = np.divide(TP, np.add(TP,FP))\n",
        "\n",
        "    # calculate accuracy\n",
        "    correct = 0.0\n",
        "    counter = 0.0\n",
        "\n",
        "    for i in range(0,len(all_predicted)):\n",
        "      if all_tags[i] == 0:\n",
        "        continue\n",
        "      counter += 1\n",
        "      if all_tags[i] == all_predicted[i]:\n",
        "        correct += 1\n",
        "    \n",
        "    accuracy = correct / counter\n",
        "    \n",
        "    # print evaluation\n",
        "    temp = np.concatenate((np.reshape(recall, (7,1)), np.reshape(precision, (7,1))), axis=1)\n",
        "    df = pd.DataFrame(data=temp, index=['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG'], columns=['recall', 'precision'])\n",
        "    print(df)\n",
        "    print(\"accuracy: \", accuracy)\n",
        "\n",
        "  # test\n",
        "  print(\"test\")\n",
        "  all_predicted = []\n",
        "  all_tags = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for sequence in test_sequences:\n",
        "      if len(sequence[0]) == 0:\n",
        "        continue\n",
        "      predicted = model(torch.LongTensor(sequence[0]).cuda()).max(1).indices\n",
        "      all_predicted.append(predicted.tolist())\n",
        "      all_tags.append(sequence[1])\n",
        "    \n",
        "    all_predicted = [prediction for sublist in all_predicted for prediction in sublist]\n",
        "    all_tags = [tag for sublist in all_tags for tag in sublist]\n",
        "\n",
        "    # calculate recall = TP/(TP+FN) = TP/P and precision = TP/(TP+FP)\n",
        "    TP = np.zeros(7)\n",
        "    P = np.zeros(7)\n",
        "    FP = np.zeros(7)\n",
        "\n",
        "    for i in range(0,len(all_predicted)):\n",
        "      P[all_tags[i]] += 1\n",
        "      if all_tags[i] == all_predicted[i]:\n",
        "        TP[all_tags[i]] += 1\n",
        "      if all_tags[i] != all_predicted[i]:\n",
        "        FP[all_predicted[i]] += 1\n",
        "    \n",
        "    recall = np.divide(TP,P)\n",
        "    precision = np.divide(TP, np.add(TP,FP))\n",
        "\n",
        "    # calculate accuracy\n",
        "    correct = 0.0\n",
        "    counter = 0.0\n",
        "\n",
        "    for i in range(0,len(all_predicted)):\n",
        "      if all_tags[i] == 0:\n",
        "        continue\n",
        "      counter += 1\n",
        "      if all_tags[i] == all_predicted[i]:\n",
        "        correct += 1\n",
        "    \n",
        "    accuracy = correct / counter\n",
        "    \n",
        "    # print evaluation\n",
        "    temp = np.concatenate((np.reshape(recall, (7,1)), np.reshape(precision, (7,1))), axis=1)\n",
        "    df = pd.DataFrame(data=temp, index=['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG'], columns=['recall', 'precision'])\n",
        "    print(df)\n",
        "    print(\"accuracy: \", accuracy, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHra37Tqe3__",
        "outputId": "ebc66447-a0dd-4a4f-d32b-3208953a14dc"
      },
      "source": [
        "input_size = len(vocab.word2id)\n",
        "output_size = 7\n",
        "embedding_size = 300\n",
        "\n",
        "model1 = NERNetGlove(input_size, embedding_size, 500, output_size, 1, 1).cuda()\n",
        "print(\"model 1 - start training\")\n",
        "train_loop_glove(model1, 10)\n",
        "\n",
        "model2 = NERNetGlove(input_size, embedding_size, 500, output_size, 2, 1).cuda()\n",
        "print(\"model 2 - start training\")\n",
        "train_loop_glove(model2, 10)\n",
        "\n",
        "model3 = NERNetGlove(input_size, embedding_size, 500, output_size, 3, 1).cuda()\n",
        "print(\"model 3 - start training\")\n",
        "train_loop_glove(model3, 10)\n",
        "\n",
        "model4 = NERNetGlove(input_size, embedding_size, 500, output_size, 1, 2).cuda()\n",
        "print(\"model 4 - start training\")\n",
        "train_loop_glove(model4, 10)\n",
        "\n",
        "model5 = NERNetGlove(input_size, embedding_size, 500, output_size, 2, 2).cuda()\n",
        "print(\"model 5 - start training\")\n",
        "train_loop_glove(model5, 10)\n",
        "\n",
        "model6 = NERNetGlove(input_size, embedding_size, 500, output_size, 3, 2).cuda()\n",
        "print(\"model 6 - start training\")\n",
        "train_loop_glove(model6, 10)\n",
        "\n",
        "model7 = NERNetGlove(input_size, embedding_size, 800, output_size, 1, 2).cuda()\n",
        "print(\"model 7 - start training\")\n",
        "train_loop_glove(model7, 10)\n",
        "'''\n",
        "model8 = NERNetGlove(input_size, embedding_size, 800, output_size, 2, 2).cuda()\n",
        "print(\"model 8 - start training\")\n",
        "train_loop_glove(model8, 10)\n",
        "'''\n",
        "model9 = NERNetGlove(input_size, embedding_size, 800, output_size, 3, 2).cuda()\n",
        "print(\"model 9 - start training\")\n",
        "train_loop_glove(model9, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model 1 - start training\n",
            "[1,   500] loss: 2.494\n",
            "[1,  1000] loss: 1.885\n",
            "[1,  1500] loss: 1.471\n",
            "[2,   500] loss: 1.237\n",
            "[2,  1000] loss: 1.205\n",
            "[2,  1500] loss: 1.084\n",
            "[3,   500] loss: 0.975\n",
            "[3,  1000] loss: 0.875\n",
            "[3,  1500] loss: 0.791\n",
            "[4,   500] loss: 0.700\n",
            "[4,  1000] loss: 0.590\n",
            "[4,  1500] loss: 0.539\n",
            "[5,   500] loss: 0.430\n",
            "[5,  1000] loss: 0.366\n",
            "[5,  1500] loss: 0.339\n",
            "[6,   500] loss: 0.257\n",
            "[6,  1000] loss: 0.257\n",
            "[6,  1500] loss: 0.223\n",
            "[7,   500] loss: 0.148\n",
            "[7,  1000] loss: 0.167\n",
            "[7,  1500] loss: 0.149\n",
            "[8,   500] loss: 0.102\n",
            "[8,  1000] loss: 0.109\n",
            "[8,  1500] loss: 0.070\n",
            "[9,   500] loss: 0.065\n",
            "[9,  1000] loss: 0.059\n",
            "[9,  1500] loss: 0.051\n",
            "[10,   500] loss: 0.039\n",
            "[10,  1000] loss: 0.042\n",
            "[10,  1500] loss: 0.037\n",
            "Finished Training\n",
            "model 2 - start training\n",
            "[1,   500] loss: 2.486\n",
            "[1,  1000] loss: 1.661\n",
            "[1,  1500] loss: 1.440\n",
            "[2,   500] loss: 1.176\n",
            "[2,  1000] loss: 1.049\n",
            "[2,  1500] loss: 0.911\n",
            "[3,   500] loss: 0.720\n",
            "[3,  1000] loss: 0.584\n",
            "[3,  1500] loss: 0.587\n",
            "[4,   500] loss: 0.423\n",
            "[4,  1000] loss: 0.427\n",
            "[4,  1500] loss: 0.361\n",
            "[5,   500] loss: 0.288\n",
            "[5,  1000] loss: 0.230\n",
            "[5,  1500] loss: 0.250\n",
            "[6,   500] loss: 0.159\n",
            "[6,  1000] loss: 0.165\n",
            "[6,  1500] loss: 0.134\n",
            "[7,   500] loss: 0.101\n",
            "[7,  1000] loss: 0.105\n",
            "[7,  1500] loss: 0.088\n",
            "[8,   500] loss: 0.049\n",
            "[8,  1000] loss: 0.048\n",
            "[8,  1500] loss: 0.059\n",
            "[9,   500] loss: 0.037\n",
            "[9,  1000] loss: 0.034\n",
            "[9,  1500] loss: 0.036\n",
            "[10,   500] loss: 0.031\n",
            "[10,  1000] loss: 0.020\n",
            "[10,  1500] loss: 0.024\n",
            "Finished Training\n",
            "model 3 - start training\n",
            "[1,   500] loss: 2.594\n",
            "[1,  1000] loss: 1.672\n",
            "[1,  1500] loss: 1.424\n",
            "[2,   500] loss: 1.130\n",
            "[2,  1000] loss: 0.963\n",
            "[2,  1500] loss: 0.816\n",
            "[3,   500] loss: 0.638\n",
            "[3,  1000] loss: 0.615\n",
            "[3,  1500] loss: 0.540\n",
            "[4,   500] loss: 0.348\n",
            "[4,  1000] loss: 0.366\n",
            "[4,  1500] loss: 0.381\n",
            "[5,   500] loss: 0.220\n",
            "[5,  1000] loss: 0.259\n",
            "[5,  1500] loss: 0.295\n",
            "[6,   500] loss: 0.155\n",
            "[6,  1000] loss: 0.202\n",
            "[6,  1500] loss: 0.201\n",
            "[7,   500] loss: 0.137\n",
            "[7,  1000] loss: 0.155\n",
            "[7,  1500] loss: 0.132\n",
            "[8,   500] loss: 0.115\n",
            "[8,  1000] loss: 0.111\n",
            "[8,  1500] loss: 0.129\n",
            "[9,   500] loss: 0.109\n",
            "[9,  1000] loss: 0.079\n",
            "[9,  1500] loss: 0.099\n",
            "[10,   500] loss: 0.073\n",
            "[10,  1000] loss: 0.105\n",
            "[10,  1500] loss: 0.065\n",
            "Finished Training\n",
            "model 4 - start training\n",
            "[1,   500] loss: 2.307\n",
            "[1,  1000] loss: 1.385\n",
            "[1,  1500] loss: 1.048\n",
            "[2,   500] loss: 0.965\n",
            "[2,  1000] loss: 0.800\n",
            "[2,  1500] loss: 0.745\n",
            "[3,   500] loss: 0.575\n",
            "[3,  1000] loss: 0.611\n",
            "[3,  1500] loss: 0.617\n",
            "[4,   500] loss: 0.462\n",
            "[4,  1000] loss: 0.391\n",
            "[4,  1500] loss: 0.365\n",
            "[5,   500] loss: 0.288\n",
            "[5,  1000] loss: 0.227\n",
            "[5,  1500] loss: 0.277\n",
            "[6,   500] loss: 0.185\n",
            "[6,  1000] loss: 0.147\n",
            "[6,  1500] loss: 0.176\n",
            "[7,   500] loss: 0.099\n",
            "[7,  1000] loss: 0.093\n",
            "[7,  1500] loss: 0.114\n",
            "[8,   500] loss: 0.069\n",
            "[8,  1000] loss: 0.065\n",
            "[8,  1500] loss: 0.056\n",
            "[9,   500] loss: 0.031\n",
            "[9,  1000] loss: 0.045\n",
            "[9,  1500] loss: 0.028\n",
            "[10,   500] loss: 0.022\n",
            "[10,  1000] loss: 0.018\n",
            "[10,  1500] loss: 0.017\n",
            "Finished Training\n",
            "model 5 - start training\n",
            "[1,   500] loss: 2.107\n",
            "[1,  1000] loss: 1.166\n",
            "[1,  1500] loss: 0.906\n",
            "[2,   500] loss: 0.667\n",
            "[2,  1000] loss: 0.626\n",
            "[2,  1500] loss: 0.614\n",
            "[3,   500] loss: 0.445\n",
            "[3,  1000] loss: 0.396\n",
            "[3,  1500] loss: 0.350\n",
            "[4,   500] loss: 0.254\n",
            "[4,  1000] loss: 0.254\n",
            "[4,  1500] loss: 0.216\n",
            "[5,   500] loss: 0.127\n",
            "[5,  1000] loss: 0.169\n",
            "[5,  1500] loss: 0.131\n",
            "[6,   500] loss: 0.089\n",
            "[6,  1000] loss: 0.074\n",
            "[6,  1500] loss: 0.092\n",
            "[7,   500] loss: 0.050\n",
            "[7,  1000] loss: 0.056\n",
            "[7,  1500] loss: 0.058\n",
            "[8,   500] loss: 0.026\n",
            "[8,  1000] loss: 0.023\n",
            "[8,  1500] loss: 0.019\n",
            "[9,   500] loss: 0.021\n",
            "[9,  1000] loss: 0.026\n",
            "[9,  1500] loss: 0.025\n",
            "[10,   500] loss: 0.007\n",
            "[10,  1000] loss: 0.008\n",
            "[10,  1500] loss: 0.029\n",
            "Finished Training\n",
            "model 6 - start training\n",
            "[1,   500] loss: 2.162\n",
            "[1,  1000] loss: 1.072\n",
            "[1,  1500] loss: 0.892\n",
            "[2,   500] loss: 0.690\n",
            "[2,  1000] loss: 0.588\n",
            "[2,  1500] loss: 0.555\n",
            "[3,   500] loss: 0.351\n",
            "[3,  1000] loss: 0.316\n",
            "[3,  1500] loss: 0.348\n",
            "[4,   500] loss: 0.201\n",
            "[4,  1000] loss: 0.189\n",
            "[4,  1500] loss: 0.215\n",
            "[5,   500] loss: 0.113\n",
            "[5,  1000] loss: 0.135\n",
            "[5,  1500] loss: 0.108\n",
            "[6,   500] loss: 0.069\n",
            "[6,  1000] loss: 0.067\n",
            "[6,  1500] loss: 0.080\n",
            "[7,   500] loss: 0.081\n",
            "[7,  1000] loss: 0.052\n",
            "[7,  1500] loss: 0.048\n",
            "[8,   500] loss: 0.028\n",
            "[8,  1000] loss: 0.026\n",
            "[8,  1500] loss: 0.042\n",
            "[9,   500] loss: 0.023\n",
            "[9,  1000] loss: 0.013\n",
            "[9,  1500] loss: 0.035\n",
            "[10,   500] loss: 0.020\n",
            "[10,  1000] loss: 0.015\n",
            "[10,  1500] loss: 0.023\n",
            "Finished Training\n",
            "model 7 - start training\n",
            "[1,   500] loss: 2.123\n",
            "[1,  1000] loss: 1.220\n",
            "[1,  1500] loss: 1.132\n",
            "[2,   500] loss: 0.800\n",
            "[2,  1000] loss: 0.823\n",
            "[2,  1500] loss: 0.786\n",
            "[3,   500] loss: 0.627\n",
            "[3,  1000] loss: 0.522\n",
            "[3,  1500] loss: 0.482\n",
            "[4,   500] loss: 0.385\n",
            "[4,  1000] loss: 0.382\n",
            "[4,  1500] loss: 0.332\n",
            "[5,   500] loss: 0.239\n",
            "[5,  1000] loss: 0.238\n",
            "[5,  1500] loss: 0.225\n",
            "[6,   500] loss: 0.165\n",
            "[6,  1000] loss: 0.144\n",
            "[6,  1500] loss: 0.129\n",
            "[7,   500] loss: 0.080\n",
            "[7,  1000] loss: 0.095\n",
            "[7,  1500] loss: 0.101\n",
            "[8,   500] loss: 0.056\n",
            "[8,  1000] loss: 0.056\n",
            "[8,  1500] loss: 0.059\n",
            "[9,   500] loss: 0.026\n",
            "[9,  1000] loss: 0.030\n",
            "[9,  1500] loss: 0.032\n",
            "[10,   500] loss: 0.015\n",
            "[10,  1000] loss: 0.019\n",
            "[10,  1500] loss: 0.025\n",
            "Finished Training\n",
            "model 9 - start training\n",
            "[1,   500] loss: 1.992\n",
            "[1,  1000] loss: 0.994\n",
            "[1,  1500] loss: 0.921\n",
            "[2,   500] loss: 0.628\n",
            "[2,  1000] loss: 0.555\n",
            "[2,  1500] loss: 0.499\n",
            "[3,   500] loss: 0.308\n",
            "[3,  1000] loss: 0.345\n",
            "[3,  1500] loss: 0.316\n",
            "[4,   500] loss: 0.210\n",
            "[4,  1000] loss: 0.166\n",
            "[4,  1500] loss: 0.207\n",
            "[5,   500] loss: 0.116\n",
            "[5,  1000] loss: 0.133\n",
            "[5,  1500] loss: 0.131\n",
            "[6,   500] loss: 0.087\n",
            "[6,  1000] loss: 0.108\n",
            "[6,  1500] loss: 0.101\n",
            "[7,   500] loss: 0.031\n",
            "[7,  1000] loss: 0.044\n",
            "[7,  1500] loss: 0.060\n",
            "[8,   500] loss: 0.042\n",
            "[8,  1000] loss: 0.047\n",
            "[8,  1500] loss: 0.050\n",
            "[9,   500] loss: 0.022\n",
            "[9,  1000] loss: 0.028\n",
            "[9,  1500] loss: 0.035\n",
            "[10,   500] loss: 0.032\n",
            "[10,  1000] loss: 0.023\n",
            "[10,  1500] loss: 0.007\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA7VDqGPjb7W",
        "outputId": "cce054b9-63bb-48ec-e9d2-120266de1ff7"
      },
      "source": [
        "evaluate_glove(model1, \"hidden_size: 500, n_layers: 1, directions: 1\")\n",
        "evaluate_glove(model2, \"hidden_size: 500, n_layers: 2, directions: 1\")\n",
        "evaluate_glove(model3, \"hidden_size: 500, n_layers: 3, directions: 1\")\n",
        "evaluate_glove(model4, \"hidden_size: 500, n_layers: 1, directions: 2\")\n",
        "evaluate_glove(model5, \"hidden_size: 500, n_layers: 2, directions: 2\")\n",
        "evaluate_glove(model6, \"hidden_size: 500, n_layers: 3, directions: 2\")\n",
        "evaluate_glove(model7, \"hidden_size: 800, n_layers: 1, directions: 2\")\n",
        "evaluate_glove(model8, \"hidden_size: 800, n_layers: 2, directions: 2\")\n",
        "evaluate_glove(model9, \"hidden_size: 800, n_layers: 3, directions: 2\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden_size: 500, n_layers: 1, directions: 1\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.968992   0.973710\n",
            "B-PER  0.805000   0.865591\n",
            "I-PER  0.847134   0.943262\n",
            "B-LOC  0.874317   0.860215\n",
            "I-LOC  0.608696   0.583333\n",
            "B-ORG  0.827381   0.646512\n",
            "I-ORG  0.655172   0.690909\n",
            "accuracy:  0.8063754427390791\n",
            "test\n",
            "         recall  precision\n",
            "O      0.968783   0.978920\n",
            "B-PER  0.831797   0.907035\n",
            "I-PER  0.847973   0.912727\n",
            "B-LOC  0.862974   0.865497\n",
            "I-LOC  0.849057   0.818182\n",
            "B-ORG  0.868571   0.660870\n",
            "I-ORG  0.740000   0.691589\n",
            "accuracy:  0.8383054892601431 \n",
            "\n",
            "hidden_size: 500, n_layers: 2, directions: 1\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.973191   0.976345\n",
            "B-PER  0.740000   0.865497\n",
            "I-PER  0.789809   0.953846\n",
            "B-LOC  0.857923   0.857923\n",
            "I-LOC  0.608696   0.583333\n",
            "B-ORG  0.803571   0.584416\n",
            "I-ORG  0.637931   0.627119\n",
            "accuracy:  0.7697756788665879\n",
            "test\n",
            "         recall  precision\n",
            "O      0.971067   0.981832\n",
            "B-PER  0.788018   0.892950\n",
            "I-PER  0.804054   0.911877\n",
            "B-LOC  0.857143   0.849711\n",
            "I-LOC  0.811321   0.781818\n",
            "B-ORG  0.840000   0.641921\n",
            "I-ORG  0.750000   0.612245\n",
            "accuracy:  0.8120525059665871 \n",
            "\n",
            "hidden_size: 500, n_layers: 3, directions: 1\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.953488   0.994274\n",
            "B-PER  0.720000   0.917197\n",
            "I-PER  0.764331   0.937500\n",
            "B-LOC  0.830601   0.628099\n",
            "I-LOC  0.652174   0.375000\n",
            "B-ORG  0.720238   0.540179\n",
            "I-ORG  0.844828   0.535519\n",
            "accuracy:  0.7674144037780402\n",
            "test\n",
            "         recall  precision\n",
            "O      0.938480   0.994995\n",
            "B-PER  0.760369   0.884718\n",
            "I-PER  0.773649   0.887597\n",
            "B-LOC  0.819242   0.623060\n",
            "I-LOC  0.754717   0.563380\n",
            "B-ORG  0.734286   0.519192\n",
            "I-ORG  0.860000   0.428928\n",
            "accuracy:  0.7810262529832935 \n",
            "\n",
            "hidden_size: 500, n_layers: 1, directions: 2\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.975452   0.975137\n",
            "B-PER  0.850000   0.821256\n",
            "I-PER  0.891720   0.886076\n",
            "B-LOC  0.868852   0.868852\n",
            "I-LOC  0.565217   0.722222\n",
            "B-ORG  0.738095   0.666667\n",
            "I-ORG  0.612069   0.755319\n",
            "accuracy:  0.7992916174734357\n",
            "test\n",
            "         recall  precision\n",
            "O      0.972133   0.982305\n",
            "B-PER  0.845622   0.855478\n",
            "I-PER  0.885135   0.879195\n",
            "B-LOC  0.868805   0.851429\n",
            "I-LOC  0.792453   0.736842\n",
            "B-ORG  0.814286   0.690073\n",
            "I-ORG  0.720000   0.730964\n",
            "accuracy:  0.834128878281623 \n",
            "\n",
            "hidden_size: 500, n_layers: 2, directions: 2\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.981266   0.973718\n",
            "B-PER  0.830000   0.813725\n",
            "I-PER  0.840764   0.949640\n",
            "B-LOC  0.846995   0.811518\n",
            "I-LOC  0.391304   0.529412\n",
            "B-ORG  0.738095   0.742515\n",
            "I-ORG  0.646552   0.714286\n",
            "accuracy:  0.7804014167650531\n",
            "test\n",
            "         recall  precision\n",
            "O      0.977920   0.981357\n",
            "B-PER  0.847926   0.895377\n",
            "I-PER  0.868243   0.934545\n",
            "B-LOC  0.862974   0.815427\n",
            "I-LOC  0.773585   0.759259\n",
            "B-ORG  0.808571   0.736979\n",
            "I-ORG  0.740000   0.698113\n",
            "accuracy:  0.831145584725537 \n",
            "\n",
            "hidden_size: 500, n_layers: 3, directions: 2\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.964793   0.991371\n",
            "B-PER  0.815000   0.915730\n",
            "I-PER  0.834395   0.935714\n",
            "B-LOC  0.836066   0.827027\n",
            "I-LOC  0.521739   0.705882\n",
            "B-ORG  0.863095   0.604167\n",
            "I-ORG  0.810345   0.552941\n",
            "accuracy:  0.8240850059031877\n",
            "test\n",
            "         recall  precision\n",
            "O      0.958124   0.994940\n",
            "B-PER  0.827189   0.947230\n",
            "I-PER  0.851351   0.913043\n",
            "B-LOC  0.880466   0.820652\n",
            "I-LOC  0.773585   0.732143\n",
            "B-ORG  0.902857   0.614786\n",
            "I-ORG  0.920000   0.564417\n",
            "accuracy:  0.8675417661097852 \n",
            "\n",
            "hidden_size: 800, n_layers: 1, directions: 2\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.974160   0.981771\n",
            "B-PER  0.875000   0.813953\n",
            "I-PER  0.917197   0.894410\n",
            "B-LOC  0.874317   0.874317\n",
            "I-LOC  0.608696   0.933333\n",
            "B-ORG  0.755952   0.705556\n",
            "I-ORG  0.750000   0.743590\n",
            "accuracy:  0.8347107438016529\n",
            "test\n",
            "         recall  precision\n",
            "O      0.973199   0.985505\n",
            "B-PER  0.857143   0.865116\n",
            "I-PER  0.888514   0.897611\n",
            "B-LOC  0.903790   0.863510\n",
            "I-LOC  0.811321   0.781818\n",
            "B-ORG  0.862857   0.780362\n",
            "I-ORG  0.810000   0.692308\n",
            "accuracy:  0.8663484486873508 \n",
            "\n",
            "hidden_size: 800, n_layers: 2, directions: 2\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.982881   0.974072\n",
            "B-PER  0.830000   0.902174\n",
            "I-PER  0.866242   0.951049\n",
            "B-LOC  0.819672   0.887574\n",
            "I-LOC  0.565217   0.590909\n",
            "B-ORG  0.791667   0.718919\n",
            "I-ORG  0.663793   0.663793\n",
            "accuracy:  0.7969303423848878\n",
            "test\n",
            "         recall  precision\n",
            "O      0.979595   0.979297\n",
            "B-PER  0.806452   0.928382\n",
            "I-PER  0.854730   0.916667\n",
            "B-LOC  0.851312   0.901235\n",
            "I-LOC  0.735849   0.764706\n",
            "B-ORG  0.831429   0.723881\n",
            "I-ORG  0.835000   0.684426\n",
            "accuracy:  0.8305489260143198 \n",
            "\n",
            "hidden_size: 800, n_layers: 3, directions: 2\n",
            "dev\n",
            "         recall  precision\n",
            "O      0.978036   0.980888\n",
            "B-PER  0.880000   0.902564\n",
            "I-PER  0.923567   0.923567\n",
            "B-LOC  0.852459   0.787879\n",
            "I-LOC  0.478261   0.687500\n",
            "B-ORG  0.755952   0.697802\n",
            "I-ORG  0.681034   0.731481\n",
            "accuracy:  0.8193624557260921\n",
            "test\n",
            "         recall  precision\n",
            "O      0.974722   0.988419\n",
            "B-PER  0.854839   0.896135\n",
            "I-PER  0.902027   0.872549\n",
            "B-LOC  0.900875   0.766749\n",
            "I-LOC  0.679245   0.837209\n",
            "B-ORG  0.785714   0.719895\n",
            "I-ORG  0.770000   0.703196\n",
            "accuracy:  0.8424821002386634 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxaESRoco6bV"
      },
      "source": [
        "**Good luck!**"
      ]
    }
  ]
}