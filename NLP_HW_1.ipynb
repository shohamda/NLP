{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP HW 1 - Gili",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ce5pQK3bFn_"
      },
      "source": [
        "# Assignment 1\n",
        "In this assignment you will be creating tools for learning and testing language models.\n",
        "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwG8v-Ll49KM"
      },
      "source": [
        "*As a preparation for this task, download the data files from the course git repository.\n",
        "\n",
        "The relevant files are under **lm-languages-data-new**:\n",
        "\n",
        "\n",
        "*   en.csv (or the equivalent JSON file)\n",
        "*   es.csv (or the equivalent JSON file)\n",
        "*   fr.csv (or the equivalent JSON file)\n",
        "*   in.csv (or the equivalent JSON file)\n",
        "*   it.csv (or the equivalent JSON file)\n",
        "*   nl.csv (or the equivalent JSON file)\n",
        "*   pt.csv (or the equivalent JSON file)\n",
        "*   tl.csv (or the equivalent JSON file)\n",
        "*   test.csv (or the equivalent JSON file)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xC-87z2GWMq",
        "outputId": "f4677e92-09a1-457b-b847-2c9778889772"
      },
      "source": [
        "!git clone https://github.com/kfirbar/nlp-course.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nlp-course'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 53 (delta 23), reused 32 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOVb4IhsqimJ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Important note: please use only the files under lm-languages-data-new and NOT under lm-languages-data**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYdhPfbAGkip",
        "outputId": "d2a7a1b0-a416-47e4-ed66-7f4ed68c03cb"
      },
      "source": [
        "!ls nlp-course/lm-languages-data-new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "en.csv\t es.json  in.csv   it.json  pt.csv    test.json   tl.csv\n",
            "en.json  fr.csv   in.json  nl.csv   pt.json   tests.csv   tl.json\n",
            "es.csv\t fr.json  it.csv   nl.json  test.csv  tests.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ashyu_mT28o6"
      },
      "source": [
        "**Part 1**\n",
        "\n",
        "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYDxkG53rfGa"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCfzsITW8Yaj"
      },
      "source": [
        "def preprocess_tweet(tweet):\n",
        "    tokens = []\n",
        "    for c in tweet:\n",
        "        tokens.append(c)\n",
        "    return tokens\n",
        "\n",
        "def unique_values_from_list(lst):\n",
        "    unique_set = set(lst)\n",
        "    unique_lst = list(unique_set)\n",
        "    return unique_lst\n",
        "\n",
        "def add_unique_symbols(text, n):\n",
        "    prefix = 'א' * (n-1) \n",
        "    suffix = 'ת' * (n-1)\n",
        "    return prefix + text + suffix\n",
        "\n",
        "def preprocess_file(file_name):\n",
        "    data_file_pd = pd.read_csv(f'/content/nlp-course/lm-languages-data-new/{file_name}')\n",
        "    tweets = data_file_pd['tweet_text']\n",
        "    tweets_tokens = []\n",
        "    for tweet in tweets:\n",
        "        tweet = add_unique_symbols(tweet, 2)\n",
        "        tweet_tokens = preprocess_tweet(tweet)\n",
        "        tweets_tokens = tweets_tokens + unique_values_from_list(tweet_tokens)\n",
        "    return unique_values_from_list(tweets_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bk9gmhr5pLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b8c755-ca95-4f82-e672-30e2afa18eeb"
      },
      "source": [
        "directory = os.fsencode('/content/nlp-course/lm-languages-data-new')\n",
        "files_tokens = []\n",
        "for file in os.listdir(directory):\n",
        "     filename = os.fsdecode(file)\n",
        "     if filename.endswith(\".csv\") and (filename not in ['test.csv', 'tests.csv']): \n",
        "         print(filename)\n",
        "         files_tokens = files_tokens + preprocess_file(filename)\n",
        "vocabulary = unique_values_from_list(files_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "es.csv\n",
            "in.csv\n",
            "en.csv\n",
            "fr.csv\n",
            "nl.csv\n",
            "it.csv\n",
            "pt.csv\n",
            "tl.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb2PGj0Yc2TY"
      },
      "source": [
        "**Part 2**\n",
        "\n",
        "Write a function lm that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
        "\n",
        "{\n",
        "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
        "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
        "}\n",
        "\n",
        "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
        "\n",
        "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrarPD4wasFV"
      },
      "source": [
        "def calculate_probas(model, vocabulary, add_one):\n",
        "    for _, counts in model.items():\n",
        "        if add_one:\n",
        "            total_counts = len(vocabulary) + sum(counts.values())\n",
        "        else:\n",
        "            total_counts = sum(counts.values())\n",
        "        for token, count in counts.items():\n",
        "            counts.update({token : count / total_counts})\n",
        "    return model\n",
        "\n",
        "def add_default_zero(model):\n",
        "    for _, counts in model.items():\n",
        "        counts.update({'default' : 0})\n",
        "    return model\n",
        "\n",
        "def add_one_func(model):\n",
        "    for _, counts in model.items():\n",
        "        for token in list(counts.items()):\n",
        "            counts.update({token[0] : token[1] + 1})\n",
        "        counts.update({'default' : 1})\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMC_u8eQbVvZ"
      },
      "source": [
        "def lm(n, vocabulary, data_file_path, add_one):\n",
        "    # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
        "    # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
        "    # data_file_path - the data_file from which we record probabilities for our model\n",
        "    # add_one - True/False (use add_one smoothing or not)\n",
        "    data = pd.read_csv(data_file_path)\n",
        "    model = {}\n",
        "    for tweet in data['tweet_text']:\n",
        "        tweet = add_unique_symbols(tweet, n)\n",
        "        for i in range(len(tweet) - n + 1):\n",
        "            ngram = tweet[i:i+n]\n",
        "            if ngram[0:n-1] not in model.keys():\n",
        "                model.update({ngram[0:n-1] : {}})\n",
        "            counts = model[ngram[0:n-1]]\n",
        "            if ngram[n-1:n] in counts.keys():\n",
        "                current_val = counts.get(ngram[n-1:n]) + 1\n",
        "            else:\n",
        "                current_val = 1\n",
        "            counts.update({ngram[n-1:n] : current_val})\n",
        "    if add_one:\n",
        "        model = add_one_func(model)\n",
        "    else:\n",
        "        model = add_default_zero(model)\n",
        "    model = calculate_probas(model, vocabulary, add_one)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8TchtI22I3"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0kkMn328-lJ"
      },
      "source": [
        "def evaluate(n, model, data_file):\n",
        "    # n - the n-gram that you used to build your model (must be the same number)\n",
        "    # model - the dictionary (model) to use for calculating perplexity\n",
        "    # data_file - the tweets file that you wish to claculate a perplexity score for\n",
        "    tweets = data_file['tweet_text']\n",
        "    total_entropy = []\n",
        "    for tweet in tweets:\n",
        "        tweet = add_unique_symbols(tweet, n)\n",
        "        entropy = []\n",
        "        for i in range(len(tweet) - n + 1):\n",
        "            ngram = tweet[i:i+n]\n",
        "            proba = model.get(ngram[0:n-1],{}).get(ngram[n-1:], model.get(ngram[0:n-1],{}).get('default',0))\n",
        "            if proba != 0:\n",
        "                entropy.append(-1 * np.log2(proba))\n",
        "        tweet_entropy = sum(entropy)\n",
        "        total_entropy.append(tweet_entropy / (len(tweet) - n + 1))\n",
        "    H = sum(total_entropy) / len(tweets)\n",
        "    perplexity = 2 ** H\n",
        "    return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enGmtLE3921p"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caAxLE9s_fvn"
      },
      "source": [
        "def match(n, add_one):\n",
        "    # n - the n-gram to use for creating n-gram models\n",
        "    # add_one - use add_one smoothing or not\n",
        "    languages = ['en','es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "    file_path = '/content/nlp-course/lm-languages-data-new/'\n",
        "    perplexity = []\n",
        "    for model_lang in languages:\n",
        "        model = lm(n, vocabulary, f'{file_path}{model_lang}.csv', add_one)\n",
        "        values = []\n",
        "        for value_lang in languages:\n",
        "            data_file = pd.read_csv(f'{file_path}{value_lang}.csv')\n",
        "            values.append(float('%.2f' % evaluate(n, model, data_file)))\n",
        "        perplexity.append(values)\n",
        "    return pd.DataFrame(perplexity, index = languages, columns = languages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waGMwA8H_n17"
      },
      "source": [
        "**Part 5**\n",
        "\n",
        "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk32naXyAMdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a966939-385e-4a5e-88f6-65f14d7434b8"
      },
      "source": [
        "for n in range(1,5):\n",
        "    for add_one in [True, False]:\n",
        "        print(f'n = {n}, add_one = {add_one}:')\n",
        "        print(match(n, add_one))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 1, add_one = True:\n",
            "       en     es     fr     in     it     nl     pt     tl\n",
            "en  38.18  39.84  41.66  41.16  40.50  39.71  41.68  43.31\n",
            "es  41.73  35.42  40.07  43.51  40.08  41.74  38.06  45.93\n",
            "fr  41.34  38.81  36.90  44.36  40.05  41.23  39.53  48.02\n",
            "in  42.08  42.19  45.77  37.12  43.11  41.78  43.68  40.92\n",
            "it  41.15  38.05  39.70  43.36  37.69  41.27  39.62  45.22\n",
            "nl  40.45  39.77  40.95  41.32  40.88  37.78  41.12  44.95\n",
            "pt  42.17  36.62  39.78  42.84  40.44  41.78  35.50  45.70\n",
            "tl  41.58  41.58  46.20  38.60  42.28  42.47  43.16  39.11\n",
            "n = 1, add_one = False:\n",
            "       en     es     fr     in     it     nl     pt     tl\n",
            "en  38.10  37.46  38.45  40.33  38.34  39.20  36.92  42.59\n",
            "es  41.11  35.34  38.04  42.54  38.50  41.01  34.57  45.06\n",
            "fr  40.56  38.14  36.82  43.41  39.32  40.63  36.71  47.15\n",
            "in  41.45  36.98  43.56  37.02  41.11  41.16  37.25  40.31\n",
            "it  40.27  37.52  38.18  42.32  37.60  40.64  35.99  44.34\n",
            "nl  39.56  38.54  40.15  40.36  39.79  37.70  38.19  44.06\n",
            "pt  41.33  36.07  38.36  41.79  38.66  40.95  35.40  44.64\n",
            "tl  40.71  40.77  43.76  37.79  40.31  41.81  41.54  39.01\n",
            "n = 2, add_one = True:\n",
            "       en     es     fr     in     it     nl     pt     tl\n",
            "en  22.04  28.41  29.49  31.79  28.90  29.34  31.38  31.19\n",
            "es  33.46  19.72  29.66  36.70  26.03  35.41  25.47  36.08\n",
            "fr  29.63  26.51  20.78  35.20  28.11  31.75  28.78  36.92\n",
            "in  30.78  29.62  34.95  22.80  30.13  31.39  32.77  27.48\n",
            "it  32.89  25.53  30.33  36.08  20.75  35.40  27.35  34.72\n",
            "nl  28.30  31.88  31.66  32.33  31.98  22.18  34.23  33.09\n",
            "pt  34.74  24.36  30.44  38.25  27.29  37.08  20.62  37.22\n",
            "tl  28.49  29.49  34.54  27.33  28.65  32.51  33.20  22.48\n",
            "n = 2, add_one = False:\n",
            "       en     es     fr     in     it     nl     pt     tl\n",
            "en  18.71  20.34  20.56  25.96  22.86  24.25  21.57  25.02\n",
            "es  26.64  16.58  21.74  28.94  20.48  28.37  19.62  28.06\n",
            "fr  24.14  19.16  17.62  28.09  22.73  25.94  21.14  28.82\n",
            "in  24.76  21.53  22.52  19.00  23.11  25.35  22.39  21.87\n",
            "it  26.57  19.19  22.33  28.52  17.38  28.59  19.80  27.10\n",
            "nl  23.21  23.22  23.64  26.35  25.40  18.82  24.29  26.25\n",
            "pt  26.61  18.93  22.33  29.32  20.67  28.86  16.88  27.88\n",
            "tl  23.01  21.27  22.37  22.22  22.08  26.15  21.69  18.49\n",
            "n = 3, add_one = True:\n",
            "       en     es     fr      in     it     nl     pt     tl\n",
            "en  27.78  50.42  48.91   78.63  58.27  62.17  55.59  70.50\n",
            "es  70.14  25.52  54.74   90.97  48.46  84.10  43.27  86.68\n",
            "fr  58.04  43.28  25.95   86.41  55.14  70.98  52.08  86.78\n",
            "in  62.18  56.63  61.84   33.68  64.70  71.80  59.93  54.62\n",
            "it  68.70  44.08  55.30   93.73  26.98  86.85  46.93  77.78\n",
            "nl  53.58  61.88  59.95   79.44  70.72  30.38  67.55  73.05\n",
            "pt  77.36  44.00  62.56  101.69  55.36  96.33  26.76  87.60\n",
            "tl  53.67  57.16  61.18   59.96  60.30  75.89  60.85  31.24\n",
            "n = 3, add_one = False:\n",
            "       en     es    fr     in     it     nl     pt     tl\n",
            "en   8.73  10.31  8.71  12.48  11.15   9.91  10.66  11.60\n",
            "es  10.56   8.44  8.99  11.09   9.56  10.20   8.82  11.21\n",
            "fr  10.40   9.57  8.47  12.31  10.69  10.45  10.64  11.90\n",
            "in  10.38  10.57  9.49   9.89  10.73  10.79  10.65   9.65\n",
            "it  10.60   9.06  9.29  11.13   8.43  10.77   9.29  10.34\n",
            "nl  10.13  11.03  9.40  12.54  11.94   9.23  11.59  11.29\n",
            "pt   9.95   8.24  8.95  10.57   9.24  10.27   8.00   9.89\n",
            "tl   9.30  10.16  8.94  10.52  10.37  10.16  10.16   8.67\n",
            "n = 4, add_one = True:\n",
            "        en      es      fr      in      it      nl      pt      tl\n",
            "en   61.69  110.46   77.31  147.50  123.30   93.48  113.34  126.28\n",
            "es  114.14   56.81   78.93  128.10   89.85  100.47   71.59  131.08\n",
            "fr  104.26   87.21   55.87  149.22  112.18  102.54  102.04  143.49\n",
            "in  114.92  125.64  101.55   83.14  130.51  121.03  125.19   98.16\n",
            "it  112.94   78.12   87.09  126.56   60.11  107.63   82.34  115.00\n",
            "nl  101.78  122.27   91.39  151.30  139.48   68.69  131.80  131.67\n",
            "pt  115.73   71.49   87.95  127.38   97.43  110.85   59.30  120.25\n",
            "tl   93.90  117.87   95.29  116.28  120.69  113.45  116.36   71.44\n",
            "n = 4, add_one = False:\n",
            "      en    es    fr    in    it    nl    pt    tl\n",
            "en  4.28  4.33  3.90  3.50  4.01  3.62  3.80  3.67\n",
            "es  3.74  4.57  3.66  3.22  4.15  3.11  3.89  3.44\n",
            "fr  4.00  4.29  4.34  3.37  4.20  3.39  4.01  3.34\n",
            "in  4.04  4.23  3.72  4.95  4.06  3.65  3.88  4.15\n",
            "it  3.69  4.43  3.73  3.15  4.43  3.06  4.13  3.36\n",
            "nl  4.22  3.95  3.77  3.59  3.87  4.46  3.67  3.56\n",
            "pt  3.39  3.90  3.51  2.92  3.83  2.89  4.27  3.15\n",
            "tl  3.93  4.36  3.62  4.18  4.19  3.45  3.92  4.33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg4h5Cl0q2nR"
      },
      "source": [
        "**Part 6**\n",
        "\n",
        "Each line in the file test.csv contains a sentence and the language it belongs to. Write a function that uses your language models to classify the correct language of each sentence.\n",
        "\n",
        "Important note regarding the grading of this section: this is an open question, where a different solution will yield different accuracy scores. any solution that is not trivial (e.g. returning 'en' in all cases) will be excepted. We do reserve the right to give bonus points to exceptionally good/creative solutions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeB2GyS4D_mx"
      },
      "source": [
        "def evaluate_tweet(n, model, tweet):\n",
        "    # n - the n-gram that you used to build your model (must be the same number)\n",
        "    # model - the dictionary (model) to use for calculating perplexity\n",
        "    # tweet - the tweetthat you wish to claculate a perplexity score for\n",
        "    tweet_text = tweet['tweet_text']\n",
        "    tweet_text = add_unique_symbols(tweet_text, n)\n",
        "    entropy = []\n",
        "    for i in range(len(tweet_text) - n + 1):\n",
        "        ngram = tweet_text[i:i+n]\n",
        "        proba = model.get(ngram[0:n-1],{}).get(ngram[n-1:], model.get(ngram[0:n-1],{}).get('default',0))\n",
        "        if proba != 0:\n",
        "            entropy.append(-1 * np.log2(proba))\n",
        "    tweet_entropy = sum(entropy) / (len(tweet_text) - n + 1)\n",
        "    tweet_perplexity = 2 ** tweet_entropy\n",
        "    return tweet_perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tgljGPqXpOe"
      },
      "source": [
        "# create all models for classification\n",
        "\n",
        "languages = ['en','es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "file_path = '/content/nlp-course/lm-languages-data-new/'\n",
        "models = {}\n",
        "\n",
        "for lang in languages:\n",
        "    for n in range(1,6):\n",
        "        for add_one in [True, False]:\n",
        "            model = lm(n, vocabulary, f'{file_path}{lang}.csv', add_one)\n",
        "            if lang in models:\n",
        "                if n in models[lang]:\n",
        "                    models[lang][n].update({add_one : model})\n",
        "                else:\n",
        "                    models[lang].update({n : {add_one : model}})\n",
        "            else:\n",
        "                models.update({lang: {n : {add_one:model}}})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAV2U8USLUXi"
      },
      "source": [
        "### Classification using majority voting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jvunc-WJ-Jt"
      },
      "source": [
        "def classify(models):\n",
        "    languages = ['en','es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "    file_path = '/content/nlp-course/lm-languages-data-new/'\n",
        "    data_file  = pd.read_csv(f'{file_path}test.csv')\n",
        "\n",
        "    final_predictions = []\n",
        "    for tweet_idx in range(0, len(data_file)):\n",
        "        tweet_predictions = []\n",
        "        tweet = data_file.iloc[tweet_idx]\n",
        "        for n in range(1, 5):\n",
        "            for add_one in [True, False]:  # [True, False]\n",
        "                perplexity = {}\n",
        "                for model_lang in languages:\n",
        "                    model = models[model_lang][n][add_one]\n",
        "                    perplexity.update({model_lang : evaluate_tweet(n, model, tweet)})\n",
        "                tweet_predictions.append(min(perplexity, key=perplexity.get))\n",
        "        final_predictions.append((tweet['tweet_id'],max(tweet_predictions,key=tweet_predictions.count)))\n",
        "    return final_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UyURCmPPuB9"
      },
      "source": [
        "final_predictions = classify(models)\n",
        "predictions_df = pd.DataFrame(final_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uodb6_FJVYJ0"
      },
      "source": [
        "labels_df = pd.read_csv('/content/nlp-course/lm-languages-data-new/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOBO3YQls66r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfbe7a16-5891-4b02-80ea-9812f7353b2c"
      },
      "source": [
        "# used macro average for balanced dataset\n",
        "from sklearn.metrics import f1_score\n",
        "f1_score(labels_df['label'],predictions_df[1], average='macro')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.873523695552922"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e5CM9hNKgEB"
      },
      "source": [
        "### Classification with predictive model (n-gram predictions as features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3rhF0WryWAc"
      },
      "source": [
        "def create_dataset(language, models, data_file):\n",
        "    # data file is a file of specific language\n",
        "    languages = ['en','es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "    final_features = []\n",
        "    for tweet_idx in range(0, len(data_file)):\n",
        "        tweet_predictions = []\n",
        "        tweet = data_file.iloc[tweet_idx]\n",
        "        tweet_predictions.append(tweet['tweet_id'])\n",
        "        for n in range(1, 6):\n",
        "            for add_one in [True, False]:  # [True, False]\n",
        "                perplexity = {}\n",
        "                for model_lang in languages:\n",
        "                    model = models[model_lang][n][add_one]\n",
        "                    perplexity.update({model_lang : evaluate_tweet(n, model, tweet)})\n",
        "                tweet_predictions.append(min(perplexity, key=perplexity.get))\n",
        "        tweet_predictions.append(language)\n",
        "        final_features.append(tweet_predictions)\n",
        "    df = pd.DataFrame(final_features, columns = ['tweet_id'] + [\"feature_\"+str(x) for x in range(1, (len(tweet_predictions) - 1))] + ['label'])\n",
        "    df.set_index(df.columns[0])\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FoE7UIeK3qm",
        "outputId": "811d1098-2ad7-45fc-91fd-87043a0dcf10"
      },
      "source": [
        "! pip install catboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/80/8e9c57ec32dfed6ba2922bc5c96462cbf8596ce1a6f5de532ad1e43e53fe/catboost-0.25.1-cp37-none-manylinux1_x86_64.whl (67.3MB)\n",
            "\u001b[K     |████████████████████████████████| 67.3MB 80kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.25.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "Po3lFsJU4Q5o",
        "outputId": "809553b1-7ab5-475b-f113-ae3e2645f646"
      },
      "source": [
        "file_path = '/content/nlp-course/lm-languages-data-new/'\n",
        "languages = ['en','es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "\n",
        "feature_matrix = None\n",
        "for lang in languages:\n",
        "    data_file  = pd.read_csv(f'{file_path}{lang}.csv')\n",
        "    df = create_dataset(lang, models, data_file)\n",
        "    feature_matrix = df if feature_matrix is None else pd.concat([feature_matrix, df]) \n",
        "feature_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "      <th>feature_10</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>845395018743459840</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>es</td>\n",
              "      <td>pt</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>845395017917173760</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>pt</td>\n",
              "      <td>it</td>\n",
              "      <td>pt</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>845395018760306693</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>pt</td>\n",
              "      <td>it</td>\n",
              "      <td>pt</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>845395018336649216</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>nl</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>845395018751856642</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>fr</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8995</th>\n",
              "      <td>829190064584392704</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>tl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8996</th>\n",
              "      <td>829190068803866625</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>fr</td>\n",
              "      <td>fr</td>\n",
              "      <td>fr</td>\n",
              "      <td>tl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8997</th>\n",
              "      <td>829190072998232065</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>pt</td>\n",
              "      <td>nl</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>fr</td>\n",
              "      <td>tl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8998</th>\n",
              "      <td>829190135883395072</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>it</td>\n",
              "      <td>en</td>\n",
              "      <td>it</td>\n",
              "      <td>tl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8999</th>\n",
              "      <td>829183726999523328</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>it</td>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>tl</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71994 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                tweet_id feature_1 feature_2  ... feature_9 feature_10 label\n",
              "0     845395018743459840        en        en  ...        es         pt    en\n",
              "1     845395017917173760        en        en  ...        it         pt    en\n",
              "2     845395018760306693        en        en  ...        it         pt    en\n",
              "3     845395018336649216        it        it  ...        tl         nl    en\n",
              "4     845395018751856642        en        en  ...        pt         fr    en\n",
              "...                  ...       ...       ...  ...       ...        ...   ...\n",
              "8995  829190064584392704        tl        tl  ...        es         es    tl\n",
              "8996  829190068803866625        tl        tl  ...        fr         fr    tl\n",
              "8997  829190072998232065        pt        pt  ...        pt         fr    tl\n",
              "8998  829190135883395072        tl        tl  ...        en         it    tl\n",
              "8999  829183726999523328        it        it  ...        in         in    tl\n",
              "\n",
              "[71994 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "juNJdvAEDXLS",
        "outputId": "5aeda9ff-80c1-4277-d039-a84cf9adda59"
      },
      "source": [
        "feature_matrix2 = feature_matrix.set_index(feature_matrix.columns[0])\n",
        "feature_matrix2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "      <th>feature_10</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tweet_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>845395018743459840</th>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>es</td>\n",
              "      <td>pt</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>845395017917173760</th>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>pt</td>\n",
              "      <td>it</td>\n",
              "      <td>pt</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>845395018760306693</th>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>pt</td>\n",
              "      <td>it</td>\n",
              "      <td>pt</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>845395018336649216</th>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>nl</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>845395018751856642</th>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>fr</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>829190064584392704</th>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>tl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>829190068803866625</th>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>fr</td>\n",
              "      <td>fr</td>\n",
              "      <td>fr</td>\n",
              "      <td>tl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>829190072998232065</th>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>pt</td>\n",
              "      <td>nl</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>fr</td>\n",
              "      <td>tl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>829190135883395072</th>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>it</td>\n",
              "      <td>en</td>\n",
              "      <td>it</td>\n",
              "      <td>tl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>829183726999523328</th>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>it</td>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>tl</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71994 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   feature_1 feature_2 feature_3  ... feature_9 feature_10 label\n",
              "tweet_id                                          ...                           \n",
              "845395018743459840        en        en        en  ...        es         pt    en\n",
              "845395017917173760        en        en        en  ...        it         pt    en\n",
              "845395018760306693        en        en        en  ...        it         pt    en\n",
              "845395018336649216        it        it        en  ...        tl         nl    en\n",
              "845395018751856642        en        en        en  ...        pt         fr    en\n",
              "...                      ...       ...       ...  ...       ...        ...   ...\n",
              "829190064584392704        tl        tl        tl  ...        es         es    tl\n",
              "829190068803866625        tl        tl        tl  ...        fr         fr    tl\n",
              "829190072998232065        pt        pt        tl  ...        pt         fr    tl\n",
              "829190135883395072        tl        tl        tl  ...        en         it    tl\n",
              "829183726999523328        it        it        tl  ...        in         in    tl\n",
              "\n",
              "[71994 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "Gp9kASXuA3s_",
        "outputId": "1a89654b-e9a5-4a9f-b2c0-c27680a34b48"
      },
      "source": [
        "file_path = '/content/nlp-course/lm-languages-data-new/'\n",
        "\n",
        "data_file  = pd.read_csv(f'{file_path}test.csv')\n",
        "test_matrix = create_dataset('test', models, data_file)\n",
        "test_matrix.merge(data_file, on='tweet_id', how='inner')\n",
        "test_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "      <th>feature_10</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>845394879479996416</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>it</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>836313846675619841</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>836259442328940544</td>\n",
              "      <td>es</td>\n",
              "      <td>in</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>fr</td>\n",
              "      <td>in</td>\n",
              "      <td>es</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>847729104472358912</td>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>in</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>836491739699412992</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>fr</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7994</th>\n",
              "      <td>836250659464761344</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>es</td>\n",
              "      <td>fr</td>\n",
              "      <td>es</td>\n",
              "      <td>pt</td>\n",
              "      <td>es</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>in</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7995</th>\n",
              "      <td>847676283089637380</td>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>it</td>\n",
              "      <td>es</td>\n",
              "      <td>en</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7996</th>\n",
              "      <td>836319299279138816</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7997</th>\n",
              "      <td>836258179847716865</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>es</td>\n",
              "      <td>pt</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>tl</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7998</th>\n",
              "      <td>845394890829807616</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>pt</td>\n",
              "      <td>en</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>it</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7999 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                tweet_id feature_1 feature_2  ... feature_9 feature_10 label\n",
              "0     845394879479996416        en        en  ...        pt         it  test\n",
              "1     836313846675619841        it        it  ...        nl         nl  test\n",
              "2     836259442328940544        es        in  ...        in         es  test\n",
              "3     847729104472358912        nl        nl  ...        it         in  test\n",
              "4     836491739699412992        tl        tl  ...        pt         fr  test\n",
              "...                  ...       ...       ...  ...       ...        ...   ...\n",
              "7994  836250659464761344        en        en  ...        pt         in  test\n",
              "7995  847676283089637380        in        in  ...        es         en  test\n",
              "7996  836319299279138816        tl        tl  ...        en         en  test\n",
              "7997  836258179847716865        pt        pt  ...        es         tl  test\n",
              "7998  845394890829807616        en        en  ...        pt         it  test\n",
              "\n",
              "[7999 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "qI73O16qEgNO",
        "outputId": "789be64b-43f6-4e8f-f5dc-58e4315814c0"
      },
      "source": [
        "test_matrix2 = test_matrix.merge(data_file, on='tweet_id', how='inner').drop(['label_x', 'tweet_text'], axis=1).rename(columns = {'label_y' : 'label'})\n",
        "test_matrix2 = test_matrix2.set_index(test_matrix2.columns[0])\n",
        "test_matrix2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "      <th>feature_10</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tweet_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>845394879479996416</th>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>it</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836313846675619841</th>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836259442328940544</th>\n",
              "      <td>es</td>\n",
              "      <td>in</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>fr</td>\n",
              "      <td>in</td>\n",
              "      <td>es</td>\n",
              "      <td>tl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>847729104472358912</th>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>in</td>\n",
              "      <td>nl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836491739699412992</th>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>fr</td>\n",
              "      <td>tl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836250659464761344</th>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>es</td>\n",
              "      <td>fr</td>\n",
              "      <td>es</td>\n",
              "      <td>pt</td>\n",
              "      <td>es</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>in</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>847676283089637380</th>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>it</td>\n",
              "      <td>es</td>\n",
              "      <td>en</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836319299279138816</th>\n",
              "      <td>tl</td>\n",
              "      <td>tl</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836258179847716865</th>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>es</td>\n",
              "      <td>pt</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>tl</td>\n",
              "      <td>pt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>845394890829807616</th>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>pt</td>\n",
              "      <td>en</td>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>it</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7999 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   feature_1 feature_2 feature_3  ... feature_9 feature_10 label\n",
              "tweet_id                                          ...                           \n",
              "845394879479996416        en        en        en  ...        pt         it    en\n",
              "836313846675619841        it        it        it  ...        nl         nl    it\n",
              "836259442328940544        es        in        tl  ...        in         es    tl\n",
              "847729104472358912        nl        nl        nl  ...        it         in    nl\n",
              "836491739699412992        tl        tl        tl  ...        pt         fr    tl\n",
              "...                      ...       ...       ...  ...       ...        ...   ...\n",
              "836250659464761344        en        en        es  ...        pt         in    es\n",
              "847676283089637380        in        in        in  ...        es         en    in\n",
              "836319299279138816        tl        tl        it  ...        en         en    it\n",
              "836258179847716865        pt        pt        pt  ...        es         tl    pt\n",
              "845394890829807616        en        en        en  ...        pt         it    en\n",
              "\n",
              "[7999 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjcMLR9f85OE"
      },
      "source": [
        "from catboost import Pool, CatBoostClassifier\n",
        "\n",
        "def catboost_classification(feature_matrix, test_matrix):\n",
        "    train = feature_matrix[feature_matrix.columns[:-1]]\n",
        "    label = feature_matrix[feature_matrix.columns[-1:]]\n",
        "    cat_features = range(0,10)\n",
        "    train_dataset = Pool(data = train, label = label, cat_features=cat_features)\n",
        "\n",
        "    model = CatBoostClassifier(iterations=100,\n",
        "                           learning_rate=0.05,\n",
        "                           loss_function='MultiClass')\n",
        "    model.fit(train_dataset)\n",
        "\n",
        "    test = test_matrix[feature_matrix.columns[:-1]]\n",
        "    label = test_matrix[feature_matrix.columns[-1:]]\n",
        "    cat_features = range(0,10)\n",
        "    test_dataset = Pool(data = test, label = label, cat_features=cat_features)\n",
        "    preds_class = model.predict(test_dataset)\n",
        "    return preds_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2gpaOKbFXFS",
        "outputId": "1667e880-7d41-4132-d19e-bcb23549d1ba"
      },
      "source": [
        "preds = catboost_classification(feature_matrix2, test_matrix2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 1.8063775\ttotal: 1.58s\tremaining: 2m 36s\n",
            "1:\tlearn: 1.6052523\ttotal: 3s\tremaining: 2m 27s\n",
            "2:\tlearn: 1.4610137\ttotal: 4.38s\tremaining: 2m 21s\n",
            "3:\tlearn: 1.3393740\ttotal: 5.75s\tremaining: 2m 17s\n",
            "4:\tlearn: 1.2439115\ttotal: 7.1s\tremaining: 2m 14s\n",
            "5:\tlearn: 1.1583623\ttotal: 8.52s\tremaining: 2m 13s\n",
            "6:\tlearn: 1.0849130\ttotal: 9.87s\tremaining: 2m 11s\n",
            "7:\tlearn: 1.0205702\ttotal: 11.2s\tremaining: 2m 9s\n",
            "8:\tlearn: 0.9644316\ttotal: 12.6s\tremaining: 2m 7s\n",
            "9:\tlearn: 0.9174021\ttotal: 14s\tremaining: 2m 5s\n",
            "10:\tlearn: 0.8750675\ttotal: 15.3s\tremaining: 2m 3s\n",
            "11:\tlearn: 0.8343682\ttotal: 16.7s\tremaining: 2m 2s\n",
            "12:\tlearn: 0.7999147\ttotal: 18.1s\tremaining: 2m 1s\n",
            "13:\tlearn: 0.7665839\ttotal: 19.5s\tremaining: 1m 59s\n",
            "14:\tlearn: 0.7365786\ttotal: 20.9s\tremaining: 1m 58s\n",
            "15:\tlearn: 0.7091301\ttotal: 22.2s\tremaining: 1m 56s\n",
            "16:\tlearn: 0.6838994\ttotal: 23.7s\tremaining: 1m 55s\n",
            "17:\tlearn: 0.6609531\ttotal: 25.1s\tremaining: 1m 54s\n",
            "18:\tlearn: 0.6398980\ttotal: 26.5s\tremaining: 1m 52s\n",
            "19:\tlearn: 0.6213680\ttotal: 27.9s\tremaining: 1m 51s\n",
            "20:\tlearn: 0.6042345\ttotal: 29.4s\tremaining: 1m 50s\n",
            "21:\tlearn: 0.5883221\ttotal: 30.7s\tremaining: 1m 49s\n",
            "22:\tlearn: 0.5730057\ttotal: 32.1s\tremaining: 1m 47s\n",
            "23:\tlearn: 0.5595010\ttotal: 33.5s\tremaining: 1m 46s\n",
            "24:\tlearn: 0.5464806\ttotal: 34.9s\tremaining: 1m 44s\n",
            "25:\tlearn: 0.5350677\ttotal: 36.3s\tremaining: 1m 43s\n",
            "26:\tlearn: 0.5243240\ttotal: 37.7s\tremaining: 1m 41s\n",
            "27:\tlearn: 0.5136926\ttotal: 39.1s\tremaining: 1m 40s\n",
            "28:\tlearn: 0.5042043\ttotal: 40.5s\tremaining: 1m 39s\n",
            "29:\tlearn: 0.4950118\ttotal: 41.8s\tremaining: 1m 37s\n",
            "30:\tlearn: 0.4868002\ttotal: 43.2s\tremaining: 1m 36s\n",
            "31:\tlearn: 0.4791773\ttotal: 44.6s\tremaining: 1m 34s\n",
            "32:\tlearn: 0.4720493\ttotal: 46s\tremaining: 1m 33s\n",
            "33:\tlearn: 0.4651171\ttotal: 47.3s\tremaining: 1m 31s\n",
            "34:\tlearn: 0.4585648\ttotal: 48.7s\tremaining: 1m 30s\n",
            "35:\tlearn: 0.4525334\ttotal: 50s\tremaining: 1m 28s\n",
            "36:\tlearn: 0.4471780\ttotal: 51.4s\tremaining: 1m 27s\n",
            "37:\tlearn: 0.4421044\ttotal: 52.8s\tremaining: 1m 26s\n",
            "38:\tlearn: 0.4374361\ttotal: 54.1s\tremaining: 1m 24s\n",
            "39:\tlearn: 0.4330780\ttotal: 55.5s\tremaining: 1m 23s\n",
            "40:\tlearn: 0.4290158\ttotal: 56.9s\tremaining: 1m 21s\n",
            "41:\tlearn: 0.4252195\ttotal: 58.2s\tremaining: 1m 20s\n",
            "42:\tlearn: 0.4215618\ttotal: 59.6s\tremaining: 1m 18s\n",
            "43:\tlearn: 0.4123847\ttotal: 1m\tremaining: 1m 17s\n",
            "44:\tlearn: 0.4061823\ttotal: 1m 2s\tremaining: 1m 16s\n",
            "45:\tlearn: 0.3994359\ttotal: 1m 3s\tremaining: 1m 14s\n",
            "46:\tlearn: 0.3936556\ttotal: 1m 5s\tremaining: 1m 13s\n",
            "47:\tlearn: 0.3884198\ttotal: 1m 6s\tremaining: 1m 11s\n",
            "48:\tlearn: 0.3834031\ttotal: 1m 7s\tremaining: 1m 10s\n",
            "49:\tlearn: 0.3790565\ttotal: 1m 9s\tremaining: 1m 9s\n",
            "50:\tlearn: 0.3749195\ttotal: 1m 10s\tremaining: 1m 7s\n",
            "51:\tlearn: 0.3710742\ttotal: 1m 12s\tremaining: 1m 6s\n",
            "52:\tlearn: 0.3669692\ttotal: 1m 13s\tremaining: 1m 5s\n",
            "53:\tlearn: 0.3634914\ttotal: 1m 14s\tremaining: 1m 3s\n",
            "54:\tlearn: 0.3600333\ttotal: 1m 16s\tremaining: 1m 2s\n",
            "55:\tlearn: 0.3569932\ttotal: 1m 17s\tremaining: 1m 1s\n",
            "56:\tlearn: 0.3540955\ttotal: 1m 19s\tremaining: 59.6s\n",
            "57:\tlearn: 0.3514293\ttotal: 1m 20s\tremaining: 58.2s\n",
            "58:\tlearn: 0.3488574\ttotal: 1m 21s\tremaining: 56.8s\n",
            "59:\tlearn: 0.3464945\ttotal: 1m 23s\tremaining: 55.5s\n",
            "60:\tlearn: 0.3442531\ttotal: 1m 24s\tremaining: 54.1s\n",
            "61:\tlearn: 0.3422707\ttotal: 1m 25s\tremaining: 52.7s\n",
            "62:\tlearn: 0.3396747\ttotal: 1m 27s\tremaining: 51.3s\n",
            "63:\tlearn: 0.3370424\ttotal: 1m 28s\tremaining: 49.9s\n",
            "64:\tlearn: 0.3341384\ttotal: 1m 30s\tremaining: 48.5s\n",
            "65:\tlearn: 0.3319107\ttotal: 1m 31s\tremaining: 47.1s\n",
            "66:\tlearn: 0.3296938\ttotal: 1m 32s\tremaining: 45.8s\n",
            "67:\tlearn: 0.3276093\ttotal: 1m 34s\tremaining: 44.4s\n",
            "68:\tlearn: 0.3254679\ttotal: 1m 35s\tremaining: 43s\n",
            "69:\tlearn: 0.3237240\ttotal: 1m 37s\tremaining: 41.6s\n",
            "70:\tlearn: 0.3218823\ttotal: 1m 38s\tremaining: 40.2s\n",
            "71:\tlearn: 0.3205127\ttotal: 1m 39s\tremaining: 38.8s\n",
            "72:\tlearn: 0.3185883\ttotal: 1m 41s\tremaining: 37.4s\n",
            "73:\tlearn: 0.3169732\ttotal: 1m 42s\tremaining: 36s\n",
            "74:\tlearn: 0.3159615\ttotal: 1m 43s\tremaining: 34.6s\n",
            "75:\tlearn: 0.3145208\ttotal: 1m 45s\tremaining: 33.2s\n",
            "76:\tlearn: 0.3131495\ttotal: 1m 46s\tremaining: 31.9s\n",
            "77:\tlearn: 0.3118263\ttotal: 1m 48s\tremaining: 30.5s\n",
            "78:\tlearn: 0.3111114\ttotal: 1m 49s\tremaining: 29.1s\n",
            "79:\tlearn: 0.3099689\ttotal: 1m 50s\tremaining: 27.7s\n",
            "80:\tlearn: 0.3090894\ttotal: 1m 52s\tremaining: 26.3s\n",
            "81:\tlearn: 0.3083691\ttotal: 1m 53s\tremaining: 24.9s\n",
            "82:\tlearn: 0.3076638\ttotal: 1m 54s\tremaining: 23.5s\n",
            "83:\tlearn: 0.3067610\ttotal: 1m 56s\tremaining: 22.2s\n",
            "84:\tlearn: 0.3060118\ttotal: 1m 57s\tremaining: 20.8s\n",
            "85:\tlearn: 0.3050487\ttotal: 1m 59s\tremaining: 19.4s\n",
            "86:\tlearn: 0.3039726\ttotal: 2m\tremaining: 18s\n",
            "87:\tlearn: 0.3023520\ttotal: 2m 1s\tremaining: 16.6s\n",
            "88:\tlearn: 0.3010718\ttotal: 2m 3s\tremaining: 15.2s\n",
            "89:\tlearn: 0.2996957\ttotal: 2m 4s\tremaining: 13.9s\n",
            "90:\tlearn: 0.2982609\ttotal: 2m 6s\tremaining: 12.5s\n",
            "91:\tlearn: 0.2972985\ttotal: 2m 7s\tremaining: 11.1s\n",
            "92:\tlearn: 0.2960779\ttotal: 2m 8s\tremaining: 9.71s\n",
            "93:\tlearn: 0.2947707\ttotal: 2m 10s\tremaining: 8.32s\n",
            "94:\tlearn: 0.2939367\ttotal: 2m 11s\tremaining: 6.93s\n",
            "95:\tlearn: 0.2929808\ttotal: 2m 13s\tremaining: 5.54s\n",
            "96:\tlearn: 0.2920758\ttotal: 2m 14s\tremaining: 4.16s\n",
            "97:\tlearn: 0.2916738\ttotal: 2m 15s\tremaining: 2.77s\n",
            "98:\tlearn: 0.2908843\ttotal: 2m 17s\tremaining: 1.39s\n",
            "99:\tlearn: 0.2904133\ttotal: 2m 18s\tremaining: 0us\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5ECmLd3rktZ"
      },
      "source": [
        "**Part 7**\n",
        "\n",
        "Calculate the F1 score of your output from part 6. (hint: you can use https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f66PzQ_zHXvp",
        "outputId": "12325a83-4498-471a-f836-6882a67c4c11"
      },
      "source": [
        "# used macro average for balanced dataset\n",
        "from sklearn.metrics import f1_score\n",
        "f1_score(test_matrix2['label'], preds, average='macro')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9155830867335729"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEtckSWNANqW"
      },
      "source": [
        "# **Good luck!**"
      ]
    }
  ]
}